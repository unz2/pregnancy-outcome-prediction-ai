{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7cfae73",
   "metadata": {},
   "source": [
    "입력 구성:\n",
    "\n",
    "범주형 컬럼 → train-fold 기준 정수 인코딩(UNK=0) → embedding\n",
    "\n",
    "연속형 컬럼 → train-fold 기준 StandardScaler\n",
    "\n",
    "(중요) 모든 인코딩/스케일링은 fold의 train만으로 fit → OOF 누수 방지\n",
    "\n",
    "모델 3개:\n",
    "\n",
    "FT-Transformer(1순위)\n",
    "\n",
    "MLP(LayerNorm)\n",
    "\n",
    "MLP(StrongReg: 높은 dropout + numeric noise)\n",
    "\n",
    "평가: StratifiedKFold 5fold로 OOF AUC 계산\n",
    "\n",
    "Seed Ensemble: SEEDS 여러 개로 돌린 뒤 OOF/TEST 평균으로 최종 성능 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10번: FT-Transformer + MLP(2종) 딥러닝 파이프라인 (OOF-safe)\n",
    "# - categorical embedding + numeric\n",
    "# - StratifiedKFold OOF AUC\n",
    "# - seed ensemble 지원\n",
    "# - outputs에 10_ 접두사 저장\n",
    "# ============================================================\n",
    "\n",
    "import os, gc, re, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "DATA_DIR = \"../data\"\n",
    "OUT_DIR  = \"../outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "EXP_PREFIX = \"10_dl_ft_mlp\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "TARGET_COL = \"임신 성공 여부\"\n",
    "ID_COL = \"ID\"\n",
    "\n",
    "N_FOLDS = 5\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 512\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 6\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "# seed ensemble\n",
    "SEEDS = [42, 123, 777]   # 원하면 [42,202,777,1024] 이런식으로 늘려도 됨\n",
    "\n",
    "# -------------------------\n",
    "# Utils\n",
    "# -------------------------\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def sigmoid_np(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# -------------------------\n",
    "# 1) Feature Engineering (AutoGluon 03 기반 그대로)\n",
    "# -------------------------\n",
    "def preprocess(df):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # 시술_대분류\n",
    "    def major_procedure(x):\n",
    "        if pd.isna(x):\n",
    "            return \"Unknown\"\n",
    "        if \"IUI\" in x: return \"IUI\"\n",
    "        if \"DI\" in x:  return \"Other\"\n",
    "        if \"ICSI\" in x:return \"ICSI\"\n",
    "        if \"IVF\" in x: return \"IVF\"\n",
    "        return \"Other\"\n",
    "    df_copy[\"시술_대분류\"] = df_copy[\"특정 시술 유형\"].apply(major_procedure)\n",
    "\n",
    "    # BLASTOCYST 포함 여부\n",
    "    df_copy[\"BLASTOCYST_포함\"] = df_copy[\"특정 시술 유형\"].astype(str).str.contains(\"BLASTOCYST\", na=False).astype(int)\n",
    "\n",
    "    # 배아 이식 여부 (구조적 결측 기반)\n",
    "    embryo_stage_cols = [\n",
    "        \"단일 배아 이식 여부\", \"착상 전 유전 진단 사용 여부\", \"배아 생성 주요 이유\",\n",
    "        \"총 생성 배아 수\", \"미세주입된 난자 수\", \"미세주입에서 생성된 배아 수\",\n",
    "        \"이식된 배아 수\", \"미세주입 배아 이식 수\", \"저장된 배아 수\",\n",
    "        \"미세주입 후 저장된 배아 수\", \"해동된 배아 수\", \"해동 난자 수\",\n",
    "        \"수집된 신선 난자 수\", \"저장된 신선 난자 수\", \"혼합된 난자 수\",\n",
    "        \"파트너 정자와 혼합된 난자 수\", \"기증자 정자와 혼합된 난자 수\",\n",
    "        \"동결 배아 사용 여부\", \"신선 배아 사용 여부\", \"기증 배아 사용 여부\", \"대리모 여부\",\n",
    "    ]\n",
    "    df_copy[\"배아_이식_미도달\"] = df_copy[embryo_stage_cols].isna().all(axis=1).astype(int)\n",
    "    df_copy[\"배아_이식_여부\"] = 1 - df_copy[\"배아_이식_미도달\"]\n",
    "\n",
    "    def embryo_stage(row):\n",
    "        if row[\"배아_이식_여부\"] == 0:\n",
    "            return \"배아단계_미도달\"\n",
    "        elif pd.isna(row[\"총 생성 배아 수\"]) or row[\"총 생성 배아 수\"] == 0:\n",
    "            return \"배아생성_실패\"\n",
    "        elif pd.isna(row[\"이식된 배아 수\"]) or row[\"이식된 배아 수\"] == 0:\n",
    "            return \"이식_미실시\"\n",
    "        else:\n",
    "            return \"이식_완료\"\n",
    "    df_copy[\"배아_진행_단계\"] = df_copy.apply(embryo_stage, axis=1)\n",
    "\n",
    "    # 총시술_bin3\n",
    "    def collapse_trials(x):\n",
    "        if x == \"0회\": return \"0회\"\n",
    "        elif x in [\"1회\",\"2회\"]: return \"1–2회\"\n",
    "        else: return \"3회 이상\"\n",
    "    df_copy[\"총시술_bin3\"] = df_copy[\"총 시술 횟수\"].apply(collapse_trials)\n",
    "\n",
    "    # 나이_3구간\n",
    "    def age_group_simple(age):\n",
    "        if age == \"알 수 없음\": return \"Unknown\"\n",
    "        elif age == \"만18-34세\": return \"34세 이하\"\n",
    "        elif age in [\"만35-37세\",\"만38-39세\"]: return \"35-39세\"\n",
    "        else: return \"40세 이상\"\n",
    "    df_copy[\"나이_3구간\"] = df_copy[\"시술 당시 나이\"].apply(age_group_simple)\n",
    "\n",
    "    # 이식배아_구간\n",
    "    def embryo_count_bin(count):\n",
    "        if pd.isna(count) or count == 0: return \"0개\"\n",
    "        elif count <= 2: return \"1-2개\"\n",
    "        else: return \"3개 이상\"\n",
    "    df_copy[\"이식배아_구간\"] = df_copy[\"이식된 배아 수\"].apply(embryo_count_bin)\n",
    "\n",
    "    # Day5_이식_여부\n",
    "    df_copy[\"Day5_이식_여부\"] = (df_copy[\"배아 이식 경과일\"] == 5.0).astype(int)\n",
    "\n",
    "    # 불임원인_복잡도\n",
    "    infertility_cols = [\n",
    "        \"남성 주 불임 원인\", \"남성 부 불임 원인\", \"여성 주 불임 원인\", \"여성 부 불임 원인\",\n",
    "        \"부부 주 불임 원인\", \"부부 부 불임 원인\", \"불명확 불임 원인\",\n",
    "        \"불임 원인 - 난관 질환\", \"불임 원인 - 남성 요인\", \"불임 원인 - 배란 장애\",\n",
    "        \"불임 원인 - 여성 요인\", \"불임 원인 - 자궁경부 문제\", \"불임 원인 - 자궁내막증\",\n",
    "        \"불임 원인 - 정자 농도\", \"불임 원인 - 정자 면역학적 요인\", \"불임 원인 - 정자 운동성\",\n",
    "        \"불임 원인 - 정자 형태\"\n",
    "    ]\n",
    "    df_copy[\"불임_원인_개수\"] = df_copy[infertility_cols].sum(axis=1)\n",
    "\n",
    "    def infertility_complexity(count):\n",
    "        if count == 0: return \"None\"\n",
    "        elif count == 1: return \"Single\"\n",
    "        elif count == 2: return \"Double\"\n",
    "        else: return \"Multiple\"\n",
    "    df_copy[\"불임원인_복잡도\"] = df_copy[\"불임_원인_개수\"].apply(infertility_complexity)\n",
    "\n",
    "    # 배아_해동_실시_여부\n",
    "    df_copy[\"배아_해동_실시_여부\"] = df_copy[\"배아 해동 경과일\"].notna().astype(int)\n",
    "\n",
    "    # 비율\n",
    "    df_copy[\"배아_생성_효율\"] = df_copy[\"총 생성 배아 수\"] / (df_copy[\"수집된 신선 난자 수\"] + 1)\n",
    "    df_copy[\"배아_이식_비율\"] = df_copy[\"이식된 배아 수\"] / (df_copy[\"총 생성 배아 수\"] + 1)\n",
    "    df_copy[\"배아_저장_비율\"] = df_copy[\"저장된 배아 수\"] / (df_copy[\"총 생성 배아 수\"] + 1)\n",
    "\n",
    "    # 교호작용\n",
    "    df_copy[\"나이×Day5\"] = df_copy[\"시술 당시 나이\"].astype(str) + \"_\" + df_copy[\"Day5_이식_여부\"].astype(str)\n",
    "    df_copy[\"시술횟수×나이\"] = df_copy[\"총시술_bin3\"].astype(str) + \"_\" + df_copy[\"나이_3구간\"].astype(str)\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "# -------------------------\n",
    "# 2) Build categorical/numeric lists\n",
    "# -------------------------\n",
    "def split_cols(df: pd.DataFrame, target_col: str):\n",
    "    feats = [c for c in df.columns if c != target_col]\n",
    "    # bool/int/float -> numeric, object/category -> categorical\n",
    "    cat_cols, num_cols = [], []\n",
    "    for c in feats:\n",
    "        if str(df[c].dtype) in [\"object\", \"category\"]:\n",
    "            cat_cols.append(c)\n",
    "        else:\n",
    "            num_cols.append(c)\n",
    "    return cat_cols, num_cols\n",
    "\n",
    "# -------------------------\n",
    "# 3) Encode categorical (TRAIN-only, fold-safe) + Scale numeric (TRAIN-only)\n",
    "# -------------------------\n",
    "def fit_transform_fold(train_df, valid_df, test_df, cat_cols, num_cols):\n",
    "    # numeric\n",
    "    scaler = StandardScaler()\n",
    "    Xtr_num = scaler.fit_transform(train_df[num_cols].astype(float))\n",
    "    Xva_num = scaler.transform(valid_df[num_cols].astype(float))\n",
    "    Xte_num = scaler.transform(test_df[num_cols].astype(float))\n",
    "\n",
    "    # categorical: factorize with train categories only, unseen -> 0\n",
    "    cat_maps = {}\n",
    "    Xtr_cat = []\n",
    "    Xva_cat = []\n",
    "    Xte_cat = []\n",
    "    cat_cardinalities = []\n",
    "\n",
    "    for c in cat_cols:\n",
    "        tr = train_df[c].astype(\"object\").fillna(\"NA\").values\n",
    "        va = valid_df[c].astype(\"object\").fillna(\"NA\").values\n",
    "        te = test_df[c].astype(\"object\").fillna(\"NA\").values\n",
    "\n",
    "        uniq = pd.unique(tr)\n",
    "        # reserve 0 for UNK, map starts at 1\n",
    "        mp = {v: i+1 for i, v in enumerate(uniq)}\n",
    "        cat_maps[c] = mp\n",
    "        card = len(mp) + 1\n",
    "        cat_cardinalities.append(card)\n",
    "\n",
    "        def enc(arr):\n",
    "            out = np.zeros(len(arr), dtype=np.int64)\n",
    "            for i, v in enumerate(arr):\n",
    "                out[i] = mp.get(v, 0)\n",
    "            return out\n",
    "\n",
    "        Xtr_cat.append(enc(tr))\n",
    "        Xva_cat.append(enc(va))\n",
    "        Xte_cat.append(enc(te))\n",
    "\n",
    "    Xtr_cat = np.stack(Xtr_cat, axis=1) if len(cat_cols) else np.zeros((len(train_df),0), dtype=np.int64)\n",
    "    Xva_cat = np.stack(Xva_cat, axis=1) if len(cat_cols) else np.zeros((len(valid_df),0), dtype=np.int64)\n",
    "    Xte_cat = np.stack(Xte_cat, axis=1) if len(cat_cols) else np.zeros((len(test_df),0), dtype=np.int64)\n",
    "\n",
    "    return (Xtr_cat, Xtr_num, Xva_cat, Xva_num, Xte_cat, Xte_num, cat_cardinalities)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Dataset\n",
    "# -------------------------\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, X_cat, X_num, y=None):\n",
    "        self.X_cat = torch.tensor(X_cat, dtype=torch.long)\n",
    "        self.X_num = torch.tensor(X_num, dtype=torch.float32)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X_cat[idx], self.X_num[idx]\n",
    "        return self.X_cat[idx], self.X_num[idx], self.y[idx]\n",
    "\n",
    "# -------------------------\n",
    "# 5) Models\n",
    "# -------------------------\n",
    "class MLPWithEmbeddings(nn.Module):\n",
    "    def __init__(self, cat_cardinalities, num_dim, emb_dim=16,\n",
    "                 hidden=[256,128], dropout=0.2, layernorm=False, noise_std=0.0):\n",
    "        super().__init__()\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "        self.embs = nn.ModuleList()\n",
    "        emb_out = 0\n",
    "        for card in cat_cardinalities:\n",
    "            d = min(emb_dim, max(2, int(round(card**0.25 * 2))))\n",
    "            self.embs.append(nn.Embedding(card, d))\n",
    "            emb_out += d\n",
    "\n",
    "        in_dim = emb_out + num_dim\n",
    "        layers = []\n",
    "        for h in hidden:\n",
    "            layers.append(nn.Linear(in_dim, h))\n",
    "            if layernorm:\n",
    "                layers.append(nn.LayerNorm(h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            in_dim = h\n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        if self.noise_std > 0 and self.training:\n",
    "            x_num = x_num + torch.randn_like(x_num) * self.noise_std\n",
    "\n",
    "        if x_cat.shape[1] > 0:\n",
    "            embs = [emb(x_cat[:,i]) for i, emb in enumerate(self.embs)]\n",
    "            x = torch.cat(embs + [x_num], dim=1)\n",
    "        else:\n",
    "            x = x_num\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "# FT-Transformer (간결 구현: tokenization + transformer encoder)\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(self, cat_cardinalities, num_dim,\n",
    "                 d_token=192, n_heads=8, n_layers=3, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_dim = num_dim\n",
    "        self.n_cat = len(cat_cardinalities)\n",
    "\n",
    "        # categorical embeddings -> token\n",
    "        self.cat_embs = nn.ModuleList()\n",
    "        for card in cat_cardinalities:\n",
    "            self.cat_embs.append(nn.Embedding(card, d_token))\n",
    "\n",
    "        # numeric -> linear to token space (각 numeric을 token으로 만들지 않고 하나의 token으로 합치는 방식)\n",
    "        self.num_proj = nn.Linear(num_dim, d_token)\n",
    "\n",
    "        # [CLS] token\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_token))\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_token, nhead=n_heads,\n",
    "            dim_feedforward=d_token*4,\n",
    "            dropout=dropout, activation=\"gelu\",\n",
    "            batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_token),\n",
    "            nn.Linear(d_token, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        bs = x_num.size(0)\n",
    "        cls = self.cls.expand(bs, 1, -1)\n",
    "\n",
    "        tokens = []\n",
    "        if self.n_cat > 0:\n",
    "            cat_tokens = [emb(x_cat[:,i]).unsqueeze(1) for i, emb in enumerate(self.cat_embs)]\n",
    "            tokens.extend(cat_tokens)\n",
    "\n",
    "        num_token = self.num_proj(x_num).unsqueeze(1)\n",
    "        tokens.append(num_token)\n",
    "\n",
    "        x = torch.cat([cls] + tokens, dim=1)  # (B, 1 + n_cat + 1, d)\n",
    "        x = self.encoder(x)\n",
    "        cls_out = x[:,0]                      # (B, d)\n",
    "        logit = self.head(cls_out).squeeze(1)\n",
    "        return logit\n",
    "\n",
    "# -------------------------\n",
    "# 6) Train/Eval loop (OOF)\n",
    "# -------------------------\n",
    "def train_one_model(model, train_loader, valid_loader, seed, lr=LR, wd=WEIGHT_DECAY):\n",
    "    model.to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    best_auc = -1\n",
    "    best_state = None\n",
    "    patience = 0\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        tr_losses = []\n",
    "        for batch in train_loader:\n",
    "            xcat, xnum, y = batch\n",
    "            xcat, xnum, y = xcat.to(DEVICE), xnum.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            logit = model(xcat, xnum)\n",
    "            loss = F.binary_cross_entropy_with_logits(logit, y)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            tr_losses.append(loss.item())\n",
    "\n",
    "        # valid\n",
    "        model.eval()\n",
    "        va_logits = []\n",
    "        va_y = []\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                xcat, xnum, y = batch\n",
    "                xcat, xnum = xcat.to(DEVICE), xnum.to(DEVICE)\n",
    "                logit = model(xcat, xnum)\n",
    "                va_logits.append(logit.detach().cpu().numpy())\n",
    "                va_y.append(y.numpy())\n",
    "        va_logits = np.concatenate(va_logits)\n",
    "        va_y = np.concatenate(va_y)\n",
    "        va_proba = sigmoid_np(va_logits)\n",
    "        auc = roc_auc_score(va_y, va_proba)\n",
    "\n",
    "        if auc > best_auc + 1e-5:\n",
    "            best_auc = auc\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= PATIENCE:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, best_auc\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_proba(model, loader):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    for batch in loader:\n",
    "        if len(batch) == 2:\n",
    "            xcat, xnum = batch\n",
    "        else:\n",
    "            xcat, xnum, _ = batch\n",
    "        xcat, xnum = xcat.to(DEVICE), xnum.to(DEVICE)\n",
    "        logit = model(xcat, xnum)\n",
    "        outs.append(logit.detach().cpu().numpy())\n",
    "    logits = np.concatenate(outs)\n",
    "    return sigmoid_np(logits)\n",
    "\n",
    "def run_cv(model_name: str, build_model_fn, df_train, df_test, cat_cols, num_cols, seeds=SEEDS):\n",
    "    y = df_train[TARGET_COL].astype(int).values\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "    oof_all_seeds = []\n",
    "    test_all_seeds = []\n",
    "    auc_all_seeds = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        set_seed(seed)\n",
    "        oof = np.zeros(len(df_train), dtype=float)\n",
    "        test_pred_seed = np.zeros(len(df_test), dtype=float)\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(skf.split(df_train, y), 1):\n",
    "            tr_df = df_train.iloc[tr_idx].reset_index(drop=True)\n",
    "            va_df = df_train.iloc[va_idx].reset_index(drop=True)\n",
    "\n",
    "            # fold-safe encoding/scaling\n",
    "            Xtr_cat, Xtr_num, Xva_cat, Xva_num, Xte_cat, Xte_num, cat_cards = fit_transform_fold(\n",
    "                tr_df, va_df, df_test, cat_cols, num_cols\n",
    "            )\n",
    "\n",
    "            tr_ds = TabDataset(Xtr_cat, Xtr_num, tr_df[TARGET_COL].astype(int).values)\n",
    "            va_ds = TabDataset(Xva_cat, Xva_num, va_df[TARGET_COL].astype(int).values)\n",
    "            te_ds = TabDataset(Xte_cat, Xte_num, None)\n",
    "\n",
    "            tr_loader = DataLoader(tr_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "            va_loader = DataLoader(va_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "            te_loader = DataLoader(te_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "            model = build_model_fn(cat_cards, Xtr_num.shape[1])\n",
    "            model, best_auc = train_one_model(model, tr_loader, va_loader, seed)\n",
    "\n",
    "            # oof\n",
    "            va_proba = predict_proba(model, va_loader)\n",
    "            oof[va_idx] = va_proba\n",
    "\n",
    "            # test (fold 평균)\n",
    "            te_proba = predict_proba(model, te_loader)\n",
    "            test_pred_seed += te_proba / N_FOLDS\n",
    "\n",
    "            del model, tr_loader, va_loader, te_loader, tr_ds, va_ds, te_ds\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        seed_auc = roc_auc_score(y, oof)\n",
    "        print(f\"[{model_name}] seed={seed} OOF AUC = {seed_auc:.6f}\")\n",
    "        auc_all_seeds.append(seed_auc)\n",
    "        oof_all_seeds.append(oof)\n",
    "        test_all_seeds.append(test_pred_seed)\n",
    "\n",
    "    # seed ensemble (mean)\n",
    "    oof_ens = np.mean(np.stack(oof_all_seeds, axis=0), axis=0)\n",
    "    test_ens = np.mean(np.stack(test_all_seeds, axis=0), axis=0)\n",
    "    ens_auc = roc_auc_score(y, oof_ens)\n",
    "    print(f\"\\n[{model_name}] SEED-ENSEMBLE OOF AUC = {ens_auc:.6f} (seeds={seeds})\\n\")\n",
    "\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"oof_by_seed\": oof_all_seeds,\n",
    "        \"test_by_seed\": test_all_seeds,\n",
    "        \"auc_by_seed\": auc_all_seeds,\n",
    "        \"oof_ens\": oof_ens,\n",
    "        \"test_ens\": test_ens,\n",
    "        \"auc_ens\": ens_auc\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# 7) Main\n",
    "# -------------------------\n",
    "print(\"=\"*80)\n",
    "print(\"Load & Preprocess\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_raw = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "test_raw  = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "sub       = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "train_df = preprocess(train_raw)\n",
    "test_df  = preprocess(test_raw)\n",
    "\n",
    "# drop ID + (03에서 제거했던) 배아_이식_미도달\n",
    "drop_cols = [ID_COL, \"배아_이식_미도달\"]\n",
    "train_df = train_df.drop(columns=[c for c in drop_cols if c in train_df.columns])\n",
    "test_df  = test_df.drop(columns=[c for c in drop_cols if c in test_df.columns])\n",
    "\n",
    "cat_cols, num_cols = split_cols(train_df, TARGET_COL)\n",
    "print(f\"cat_cols={len(cat_cols)}, num_cols={len(num_cols)}\")\n",
    "print(\"target rate:\", train_df[TARGET_COL].mean())\n",
    "\n",
    "# -------------------------\n",
    "# (B) FT-Transformer\n",
    "# -------------------------\n",
    "def build_ft(cat_cards, num_dim):\n",
    "    return FTTransformer(\n",
    "        cat_cardinalities=cat_cards,\n",
    "        num_dim=num_dim,\n",
    "        d_token=192,\n",
    "        n_heads=8,\n",
    "        n_layers=3,\n",
    "        dropout=0.1\n",
    "    )\n",
    "\n",
    "res_ft = run_cv(\"FTTransformer\", build_ft, train_df, test_df, cat_cols, num_cols, seeds=SEEDS)\n",
    "\n",
    "# -------------------------\n",
    "# (C-1) MLP LayerNorm형\n",
    "# -------------------------\n",
    "def build_mlp_ln(cat_cards, num_dim):\n",
    "    return MLPWithEmbeddings(\n",
    "        cat_cardinalities=cat_cards,\n",
    "        num_dim=num_dim,\n",
    "        emb_dim=16,\n",
    "        hidden=[512, 256, 128],\n",
    "        dropout=0.25,\n",
    "        layernorm=True,\n",
    "        noise_std=0.0\n",
    "    )\n",
    "\n",
    "res_mlp_ln = run_cv(\"MLP_LayerNorm\", build_mlp_ln, train_df, test_df, cat_cols, num_cols, seeds=SEEDS)\n",
    "\n",
    "# -------------------------\n",
    "# (C-2) MLP StrongReg형\n",
    "# -------------------------\n",
    "def build_mlp_reg(cat_cards, num_dim):\n",
    "    return MLPWithEmbeddings(\n",
    "        cat_cardinalities=cat_cards,\n",
    "        num_dim=num_dim,\n",
    "        emb_dim=16,\n",
    "        hidden=[512, 256, 128],\n",
    "        dropout=0.45,     # 강한 드롭아웃\n",
    "        layernorm=False,\n",
    "        noise_std=0.03    # numeric gaussian noise (train only)\n",
    "    )\n",
    "\n",
    "res_mlp_reg = run_cv(\"MLP_StrongReg\", build_mlp_reg, train_df, test_df, cat_cols, num_cols, seeds=SEEDS)\n",
    "\n",
    "# -------------------------\n",
    "# 8) Save outputs (10_ prefix)\n",
    "# -------------------------\n",
    "def save_pack(res):\n",
    "    name = res[\"model_name\"]\n",
    "    # OOF / TEST (ensemble)\n",
    "    oof_path = os.path.join(OUT_DIR, f\"10_oof_{EXP_PREFIX}_{name}.npy\")\n",
    "    test_path= os.path.join(OUT_DIR, f\"10_test_{EXP_PREFIX}_{name}.npy\")\n",
    "    np.save(oof_path, res[\"oof_ens\"])\n",
    "    np.save(test_path, res[\"test_ens\"])\n",
    "\n",
    "    # submission\n",
    "    sub2 = sub.copy()\n",
    "    sub2[\"probability\"] = res[\"test_ens\"]\n",
    "    sub_path = os.path.join(OUT_DIR, f\"10_submission_{EXP_PREFIX}_{name}_AUC{res['auc_ens']:.6f}.csv\")\n",
    "    sub2.to_csv(sub_path, index=False)\n",
    "\n",
    "    # summary\n",
    "    summ_path = os.path.join(OUT_DIR, f\"10_summary_{EXP_PREFIX}_{name}.txt\")\n",
    "    with open(summ_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"model={name}\\n\")\n",
    "        f.write(f\"seeds={SEEDS}\\n\")\n",
    "        f.write(f\"folds={N_FOLDS}\\n\")\n",
    "        f.write(f\"ensemble_oof_auc={res['auc_ens']:.6f}\\n\")\n",
    "        f.write(f\"auc_by_seed={res['auc_by_seed']}\\n\")\n",
    "        f.write(f\"cat_cols={len(cat_cols)}, num_cols={len(num_cols)}\\n\")\n",
    "    print(f\"[saved] {name}:\")\n",
    "    print(\" -\", oof_path)\n",
    "    print(\" -\", test_path)\n",
    "    print(\" -\", sub_path)\n",
    "    print(\" -\", summ_path)\n",
    "\n",
    "save_pack(res_ft)\n",
    "save_pack(res_mlp_ln)\n",
    "save_pack(res_mlp_reg)\n",
    "\n",
    "print(\"\\nDONE. outputs 폴더에 10_ 파일들 생성 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3290d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fertility_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
