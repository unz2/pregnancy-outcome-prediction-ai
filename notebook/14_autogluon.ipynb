{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6538485c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "14번: AutoGluon + KNN Imputation + 12번 피처셋\n",
      "================================================================================\n",
      "\n",
      "[Step 1] Imputation...\n",
      "  KNN Imputing 3 columns...\n",
      "  Done.\n",
      "\n",
      "[Step 2] Feature Engineering...\n",
      "Features: 128\n",
      "\n",
      "[Step 3] AutoGluon OOF (5-fold)\n",
      "  presets=best_quality, time_limit=7200s/fold\n",
      "============================================================\n",
      "\n",
      "--- Fold 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 7200 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: ../outputs/ag_fold1/ds_sub_fit/sub_fit_ho.\n",
      "2026-02-11 12:28:58,827\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "Running the sub-fit in a ray process to avoid memory leakage.\n",
      "Spend 1875 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 5325 seconds.\n",
      "Starting full fit now with num_stack_levels 0.\n",
      "Beginning AutoGluon training ... Time limit = 5325s\n",
      "AutoGluon will save models to \"../outputs/ag_fold1\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.11.14\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:29 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       4.50 GB / 16.00 GB (28.1%)\n",
      "Disk Space Avail:   195.97 GB / 460.43 GB (42.6%)\n",
      "===================================================\n",
      "Train Data Rows:    205080\n",
      "Train Data Columns: 128\n",
      "Label Column:       임신 성공 여부\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5131.96 MB\n",
      "\tTrain Data (Original)  Memory Usage: 871.27 MB (17.0% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 17.0% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 35 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 3): ['불임 원인 - 여성 요인', '배아_이식_여부', '배아_해동_실시_여부']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 6): ['불임 원인 - 정자 면역학적 요인', '불임 원인 - 정자 운동성', '나이_알수없음', '신선_배양시간', 'PGD_실시', 'PGS_실시']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['신선_배양시간']\n",
      "\t\t('int', [])   : 5 | ['불임 원인 - 정자 면역학적 요인', '불임 원인 - 정자 운동성', '나이_알수없음', 'PGD_실시', 'PGS_실시']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 46 | ['임신 시도 또는 마지막 임신 경과 연수', '단일 배아 이식 여부', '착상 전 유전 검사 사용 여부', '착상 전 유전 진단 사용 여부', '총 생성 배아 수', ...]\n",
      "\t\t('int', [])    : 33 | ['배란 자극 여부', '남성 주 불임 원인', '남성 부 불임 원인', '여성 주 불임 원인', '여성 부 불임 원인', ...]\n",
      "\t\t('object', []) : 40 | ['시술 시기 코드', '시술 당시 나이', '시술 유형', '특정 시술 유형', '배란 유도 유형', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 38 | ['시술 시기 코드', '시술 당시 나이', '특정 시술 유형', '배아 생성 주요 이유', '총 시술 횟수', ...]\n",
      "\t\t('float', [])     : 38 | ['임신 시도 또는 마지막 임신 경과 연수', '착상 전 유전 진단 사용 여부', '총 생성 배아 수', '미세주입된 난자 수', '미세주입에서 생성된 배아 수', ...]\n",
      "\t\t('int', [])       : 13 | ['불임_원인_개수', '총시술_ord', 'IVF시술_ord', '클리닉시술_ord', 'DI시술_ord', ...]\n",
      "\t\t('int', ['bool']) : 30 | ['시술 유형', '배란 자극 여부', '배란 유도 유형', '단일 배아 이식 여부', '착상 전 유전 검사 사용 여부', ...]\n",
      "\t7.5s = Fit runtime\n",
      "\t119 features in original data used to generate 119 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 93.11 MB (1.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 8.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 5316.86s of the 5316.85s of remaining time.\n",
      "\t0.6332\t = Validation score   (roc_auc)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t63.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 5252.77s of the 5252.76s of remaining time.\n",
      "\t0.6196\t = Validation score   (roc_auc)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t64.52s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5187.14s of the 5187.13s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.71% memory usage per fold, 62.83%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=15.71%)\n",
      "\t0.7399\t = Validation score   (roc_auc)\n",
      "\t34.94s\t = Training   runtime\n",
      "\t3.93s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 5147.47s of the 5147.47s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.63% memory usage per fold, 62.54%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=15.63%)\n",
      "\t0.7396\t = Validation score   (roc_auc)\n",
      "\t31.16s\t = Training   runtime\n",
      "\t2.12s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 5112.65s of the 5112.64s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 280 due to low memory. Expected memory usage reduced from 16.05% -> 15.0% of available memory...\n",
      "\t0.7268\t = Validation score   (roc_auc)\n",
      "\t31.79s\t = Training   runtime\n",
      "\t14.5s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 5065.09s of the 5065.08s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 237 due to low memory. Expected memory usage reduced from 18.97% -> 15.0% of available memory...\n",
      "\t0.7271\t = Validation score   (roc_auc)\n",
      "\t28.17s\t = Training   runtime\n",
      "\t15.61s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 5019.9s of the 5019.89s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.97% memory usage per fold, 71.87%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=17.97%)\n",
      "\t0.7407\t = Validation score   (roc_auc)\n",
      "\t3543.07s\t = Training   runtime\n",
      "\t0.67s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 1472.56s of the 1472.56s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 287 due to low memory. Expected memory usage reduced from 15.67% -> 15.0% of available memory...\n",
      "\t0.7298\t = Validation score   (roc_auc)\n",
      "\t25.68s\t = Training   runtime\n",
      "\t14.99s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 1430.85s of the 1430.84s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 271 due to low memory. Expected memory usage reduced from 16.59% -> 15.0% of available memory...\n",
      "\t0.7304\t = Validation score   (roc_auc)\n",
      "\t24.75s\t = Training   runtime\n",
      "\t15.82s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1389.04s of the 1389.04s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.43% memory usage per fold, 48.87%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=24.43%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=83184, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 404, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 855, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 358, in _fit\n",
      "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
      "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 272, in fit\n",
      "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 207, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "                                        ^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 261, in _do_fit\n",
      "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 209, in _with_events\n",
      "    self(f'after_{event_type}');  final()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 180, in __call__\n",
      "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastcore/foundation.py\", line 163, in map\n",
      "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
      "                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastcore/basics.py\", line 934, in map_ex\n",
      "    return list(res)\n",
      "           ^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastcore/basics.py\", line 919, in __call__\n",
      "    return self.func(*fargs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 184, in _call_one\n",
      "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
      "                                        ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/callback/core.py\", line 64, in __call__\n",
      "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/callback/core.py\", line 62, in __call__\n",
      "    try: res = getcallable(self, event_name)()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
      "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
      "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
      "\tWARNING: NaN loss encountered in epoch 0!\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 855, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 286, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 717, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 663, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 605, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 568, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 534, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "                                                                                         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/ray/_private/worker.py\", line 2667, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=83184, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 404, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 855, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 358, in _fit\n",
      "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
      "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 272, in fit\n",
      "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 207, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "                                        ^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 261, in _do_fit\n",
      "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 209, in _with_events\n",
      "    self(f'after_{event_type}');  final()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 180, in __call__\n",
      "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastcore/foundation.py\", line 163, in map\n",
      "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
      "                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastcore/basics.py\", line 934, in map_ex\n",
      "    return list(res)\n",
      "           ^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastcore/basics.py\", line 919, in __call__\n",
      "    return self.func(*fargs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 184, in _call_one\n",
      "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
      "                                        ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/callback/core.py\", line 64, in __call__\n",
      "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/callback/core.py\", line 62, in __call__\n",
      "    try: res = getcallable(self, event_name)()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
      "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
      "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
      "\tWARNING: NaN loss encountered in epoch 0!\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1131.77s of the 1131.76s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 19.11% memory usage per fold, 76.44%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=19.11%)\n",
      "2026-02-11 14:10:13,082\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t0.7397\t = Validation score   (roc_auc)\n",
      "\t917.05s\t = Training   runtime\n",
      "\t4.75s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 210.74s of the 210.73s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.55% memory usage per fold, 50.20%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=12.55%)\n",
      "\t0.7368\t = Validation score   (roc_auc)\n",
      "\t168.27s\t = Training   runtime\n",
      "\t10.22s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 37.21s of the 37.2s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.86% memory usage per fold, 67.45%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=16.86%)\n",
      "\t0.7381\t = Validation score   (roc_auc)\n",
      "\t35.75s\t = Training   runtime\n",
      "\t5.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 531.69s of the -3.41s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.5, 'XGBoost_BAG_L1': 0.2, 'LightGBMXT_BAG_L1': 0.15, 'NeuralNetTorch_BAG_L1': 0.1, 'ExtraTreesEntr_BAG_L1': 0.05}\n",
      "\t0.7411\t = Validation score   (roc_auc)\n",
      "\t26.5s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5355.07s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../outputs/ag_fold1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 1 AUC: 0.738935\n",
      "  Best model: WeightedEnsemble_L2 (score=0.741056)\n",
      "\n",
      "--- Fold 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 7200 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: ../outputs/ag_fold2/ds_sub_fit/sub_fit_ho.\n",
      "Running the sub-fit in a ray process to avoid memory leakage.\n",
      "Spend 1869 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 5331 seconds.\n",
      "Starting full fit now with num_stack_levels 0.\n",
      "Beginning AutoGluon training ... Time limit = 5331s\n",
      "AutoGluon will save models to \"../outputs/ag_fold2\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.11.14\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:29 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       4.67 GB / 16.00 GB (29.2%)\n",
      "Disk Space Avail:   193.25 GB / 460.43 GB (42.0%)\n",
      "===================================================\n",
      "Train Data Rows:    205081\n",
      "Train Data Columns: 128\n",
      "Label Column:       임신 성공 여부\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5198.38 MB\n",
      "\tTrain Data (Original)  Memory Usage: 871.27 MB (16.8% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 16.8% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 35 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 3): ['불임 원인 - 여성 요인', '배아_이식_여부', '배아_해동_실시_여부']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 5): ['불임 원인 - 정자 면역학적 요인', '불임 원인 - 정자 형태', '신선_배양시간', 'PGD_실시', 'PGS_실시']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['신선_배양시간']\n",
      "\t\t('int', [])   : 4 | ['불임 원인 - 정자 면역학적 요인', '불임 원인 - 정자 형태', 'PGD_실시', 'PGS_실시']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 46 | ['임신 시도 또는 마지막 임신 경과 연수', '단일 배아 이식 여부', '착상 전 유전 검사 사용 여부', '착상 전 유전 진단 사용 여부', '총 생성 배아 수', ...]\n",
      "\t\t('int', [])    : 34 | ['배란 자극 여부', '남성 주 불임 원인', '남성 부 불임 원인', '여성 주 불임 원인', '여성 부 불임 원인', ...]\n",
      "\t\t('object', []) : 40 | ['시술 시기 코드', '시술 당시 나이', '시술 유형', '특정 시술 유형', '배란 유도 유형', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 38 | ['시술 시기 코드', '시술 당시 나이', '특정 시술 유형', '배아 생성 주요 이유', '총 시술 횟수', ...]\n",
      "\t\t('float', [])     : 38 | ['임신 시도 또는 마지막 임신 경과 연수', '착상 전 유전 진단 사용 여부', '총 생성 배아 수', '미세주입된 난자 수', '미세주입에서 생성된 배아 수', ...]\n",
      "\t\t('int', [])       : 13 | ['불임_원인_개수', '총시술_ord', 'IVF시술_ord', '클리닉시술_ord', 'DI시술_ord', ...]\n",
      "\t\t('int', ['bool']) : 31 | ['시술 유형', '배란 자극 여부', '배란 유도 유형', '단일 배아 이식 여부', '착상 전 유전 검사 사용 여부', ...]\n",
      "\t6.5s = Fit runtime\n",
      "\t120 features in original data used to generate 120 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 93.31 MB (1.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 7.25s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 5323.75s of the 5323.72s of remaining time.\n",
      "\t0.6333\t = Validation score   (roc_auc)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t63.6s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 5259.33s of the 5259.31s of remaining time.\n",
      "\t0.6196\t = Validation score   (roc_auc)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t61.17s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5196.99s of the 5196.96s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 13.58% memory usage per fold, 54.33%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=13.58%)\n",
      "\t0.7388\t = Validation score   (roc_auc)\n",
      "\t34.44s\t = Training   runtime\n",
      "\t4.35s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 5157.67s of the 5157.65s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 13.52% memory usage per fold, 54.08%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=13.52%)\n",
      "\t0.7383\t = Validation score   (roc_auc)\n",
      "\t36.02s\t = Training   runtime\n",
      "\t3.49s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 5117.01s of the 5116.98s of remaining time.\n",
      "\t0.7257\t = Validation score   (roc_auc)\n",
      "\t33.92s\t = Training   runtime\n",
      "\t18.97s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 5062.69s of the 5062.66s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 275 due to low memory. Expected memory usage reduced from 16.32% -> 15.0% of available memory...\n",
      "\t0.7261\t = Validation score   (roc_auc)\n",
      "\t31.53s\t = Training   runtime\n",
      "\t19.3s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 5010.63s of the 5010.6s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.41% memory usage per fold, 61.64%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=15.41%)\n",
      "\t0.7394\t = Validation score   (roc_auc)\n",
      "\t3550.98s\t = Training   runtime\n",
      "\t0.67s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 1455.44s of the 1455.41s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 287 due to low memory. Expected memory usage reduced from 15.66% -> 15.0% of available memory...\n",
      "\t0.7284\t = Validation score   (roc_auc)\n",
      "\t27.29s\t = Training   runtime\n",
      "\t20.16s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 1406.43s of the 1406.4s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 256 due to low memory. Expected memory usage reduced from 17.58% -> 15.0% of available memory...\n",
      "\t0.7286\t = Validation score   (roc_auc)\n",
      "\t24.99s\t = Training   runtime\n",
      "\t19.24s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1360.68s of the 1360.65s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 26.39% memory usage per fold, 52.77%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=26.39%)\n",
      "\t0.7367\t = Validation score   (roc_auc)\n",
      "\t953.15s\t = Training   runtime\n",
      "\t7.37s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 402.61s of the 402.58s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.99% memory usage per fold, 41.99%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=20.99%)\n",
      "\t0.7365\t = Validation score   (roc_auc)\n",
      "\t340.03s\t = Training   runtime\n",
      "\t3.25s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 58.06s of the 58.03s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.84% memory usage per fold, 51.34%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=12.84%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "2026-02-11 16:31:12,370\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 38.62s of the 38.6s of remaining time.\n",
      "2026-02-11 16:31:12,376\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-02-11 16:31:12,379\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-02-11 16:31:12,382\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.43% memory usage per fold, 65.71%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=16.43%)\n",
      "\t0.7363\t = Validation score   (roc_auc)\n",
      "\t37.95s\t = Training   runtime\n",
      "\t5.21s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 532.37s of the -3.36s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.48, 'LightGBMXT_BAG_L1': 0.24, 'NeuralNetFastAI_BAG_L1': 0.2, 'RandomForestGini_BAG_L1': 0.04, 'XGBoost_BAG_L1': 0.04}\n",
      "\t0.7399\t = Validation score   (roc_auc)\n",
      "\t28.2s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5362.76s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../outputs/ag_fold2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 2 AUC: 0.743414\n",
      "  Best model: CatBoost_BAG_L1 (score=0.739364)\n",
      "\n",
      "--- Fold 3/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 7200 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: ../outputs/ag_fold3/ds_sub_fit/sub_fit_ho.\n",
      "Running the sub-fit in a ray process to avoid memory leakage.\n",
      "Spend 1877 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 5323 seconds.\n",
      "Starting full fit now with num_stack_levels 0.\n",
      "Beginning AutoGluon training ... Time limit = 5323s\n",
      "AutoGluon will save models to \"../outputs/ag_fold3\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.11.14\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:29 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       4.58 GB / 16.00 GB (28.6%)\n",
      "Disk Space Avail:   189.92 GB / 460.43 GB (41.2%)\n",
      "===================================================\n",
      "Train Data Rows:    205081\n",
      "Train Data Columns: 128\n",
      "Label Column:       임신 성공 여부\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5052.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 871.28 MB (17.2% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 17.2% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 34 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 4): ['불임 원인 - 여성 요인', '불임 원인 - 정자 면역학적 요인', '배아_이식_여부', '배아_해동_실시_여부']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 4): ['불임 원인 - 정자 운동성', '신선_배양시간', 'PGD_실시', 'PGS_실시']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['신선_배양시간']\n",
      "\t\t('int', [])   : 3 | ['불임 원인 - 정자 운동성', 'PGD_실시', 'PGS_실시']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 46 | ['임신 시도 또는 마지막 임신 경과 연수', '단일 배아 이식 여부', '착상 전 유전 검사 사용 여부', '착상 전 유전 진단 사용 여부', '총 생성 배아 수', ...]\n",
      "\t\t('int', [])    : 34 | ['배란 자극 여부', '남성 주 불임 원인', '남성 부 불임 원인', '여성 주 불임 원인', '여성 부 불임 원인', ...]\n",
      "\t\t('object', []) : 40 | ['시술 시기 코드', '시술 당시 나이', '시술 유형', '특정 시술 유형', '배란 유도 유형', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 38 | ['시술 시기 코드', '시술 당시 나이', '특정 시술 유형', '배아 생성 주요 이유', '총 시술 횟수', ...]\n",
      "\t\t('float', [])     : 38 | ['임신 시도 또는 마지막 임신 경과 연수', '착상 전 유전 진단 사용 여부', '총 생성 배아 수', '미세주입된 난자 수', '미세주입에서 생성된 배아 수', ...]\n",
      "\t\t('int', [])       : 13 | ['불임_원인_개수', '총시술_ord', 'IVF시술_ord', '클리닉시술_ord', 'DI시술_ord', ...]\n",
      "\t\t('int', ['bool']) : 31 | ['시술 유형', '배란 자극 여부', '배란 유도 유형', '단일 배아 이식 여부', '착상 전 유전 검사 사용 여부', ...]\n",
      "\t8.0s = Fit runtime\n",
      "\t120 features in original data used to generate 120 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 93.31 MB (1.7% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 8.45s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 5314.55s of the 5314.54s of remaining time.\n",
      "\t0.634\t = Validation score   (roc_auc)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t62.33s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 5251.09s of the 5251.09s of remaining time.\n",
      "\t0.6197\t = Validation score   (roc_auc)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t62.43s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5187.57s of the 5187.57s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.18% memory usage per fold, 56.70%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=14.18%)\n",
      "\t0.739\t = Validation score   (roc_auc)\n",
      "\t31.51s\t = Training   runtime\n",
      "\t3.51s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 5151.33s of the 5151.32s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.90% memory usage per fold, 51.60%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=12.90%)\n",
      "\t0.7387\t = Validation score   (roc_auc)\n",
      "\t31.51s\t = Training   runtime\n",
      "\t3.9s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 5115.16s of the 5115.15s of remaining time.\n",
      "\t0.7257\t = Validation score   (roc_auc)\n",
      "\t36.59s\t = Training   runtime\n",
      "\t21.31s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 5055.82s of the 5055.81s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 265 due to low memory. Expected memory usage reduced from 16.93% -> 15.0% of available memory...\n",
      "\t0.7254\t = Validation score   (roc_auc)\n",
      "\t34.54s\t = Training   runtime\n",
      "\t20.31s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 4999.48s of the 4999.48s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.47% memory usage per fold, 61.88%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=15.47%)\n",
      "\t0.7402\t = Validation score   (roc_auc)\n",
      "\t3937.87s\t = Training   runtime\n",
      "\t0.87s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 1056.91s of the 1056.91s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 253 due to low memory. Expected memory usage reduced from 17.76% -> 15.0% of available memory...\n",
      "\t0.7286\t = Validation score   (roc_auc)\n",
      "\t23.26s\t = Training   runtime\n",
      "\t18.24s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 1013.98s of the 1013.98s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 223 due to low memory. Expected memory usage reduced from 20.13% -> 15.0% of available memory...\n",
      "\t0.7287\t = Validation score   (roc_auc)\n",
      "\t21.74s\t = Training   runtime\n",
      "\t15.53s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 975.61s of the 975.6s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 29.48% memory usage per fold, 58.97%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=29.48%)\n",
      "\t0.738\t = Validation score   (roc_auc)\n",
      "\t665.83s\t = Training   runtime\n",
      "\t7.19s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 304.85s of the 304.84s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.77% memory usage per fold, 49.54%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=24.77%)\n",
      "\t0.736\t = Validation score   (roc_auc)\n",
      "\t261.82s\t = Training   runtime\n",
      "\t3.43s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 38.63s of the 38.62s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.95% memory usage per fold, 59.82%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=14.95%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "2026-02-11 18:34:32,071\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-02-11 18:34:32,075\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 17.57s of the 17.57s of remaining time.\n",
      "2026-02-11 18:34:32,077\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-02-11 18:34:32,081\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.91% memory usage per fold, 67.65%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=16.91%)\n",
      "\t0.7349\t = Validation score   (roc_auc)\n",
      "\t20.16s\t = Training   runtime\n",
      "\t1.52s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 531.45s of the -6.71s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.64, 'NeuralNetFastAI_BAG_L1': 0.2, 'LightGBMXT_BAG_L1': 0.08, 'LightGBM_BAG_L1': 0.04, 'ExtraTreesEntr_BAG_L1': 0.04}\n",
      "\t0.7405\t = Validation score   (roc_auc)\n",
      "\t27.0s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5356.91s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../outputs/ag_fold3\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 3 AUC: 0.740964\n",
      "  Best model: WeightedEnsemble_L2 (score=0.740456)\n",
      "\n",
      "--- Fold 4/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 7200 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: ../outputs/ag_fold4/ds_sub_fit/sub_fit_ho.\n",
      "Running the sub-fit in a ray process to avoid memory leakage.\n",
      "Spend 1855 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 5345 seconds.\n",
      "Starting full fit now with num_stack_levels 0.\n",
      "Beginning AutoGluon training ... Time limit = 5345s\n",
      "AutoGluon will save models to \"../outputs/ag_fold4\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.11.14\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:29 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       4.92 GB / 16.00 GB (30.8%)\n",
      "Disk Space Avail:   185.39 GB / 460.43 GB (40.3%)\n",
      "===================================================\n",
      "Train Data Rows:    205081\n",
      "Train Data Columns: 128\n",
      "Label Column:       임신 성공 여부\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5459.30 MB\n",
      "\tTrain Data (Original)  Memory Usage: 871.28 MB (16.0% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 16.0% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 35 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 3): ['불임 원인 - 여성 요인', '배아_이식_여부', '배아_해동_실시_여부']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 4): ['불임 원인 - 정자 면역학적 요인', '신선_배양시간', 'PGD_실시', 'PGS_실시']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['신선_배양시간']\n",
      "\t\t('int', [])   : 3 | ['불임 원인 - 정자 면역학적 요인', 'PGD_실시', 'PGS_실시']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 46 | ['임신 시도 또는 마지막 임신 경과 연수', '단일 배아 이식 여부', '착상 전 유전 검사 사용 여부', '착상 전 유전 진단 사용 여부', '총 생성 배아 수', ...]\n",
      "\t\t('int', [])    : 35 | ['배란 자극 여부', '남성 주 불임 원인', '남성 부 불임 원인', '여성 주 불임 원인', '여성 부 불임 원인', ...]\n",
      "\t\t('object', []) : 40 | ['시술 시기 코드', '시술 당시 나이', '시술 유형', '특정 시술 유형', '배란 유도 유형', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 38 | ['시술 시기 코드', '시술 당시 나이', '특정 시술 유형', '배아 생성 주요 이유', '총 시술 횟수', ...]\n",
      "\t\t('float', [])     : 38 | ['임신 시도 또는 마지막 임신 경과 연수', '착상 전 유전 진단 사용 여부', '총 생성 배아 수', '미세주입된 난자 수', '미세주입에서 생성된 배아 수', ...]\n",
      "\t\t('int', [])       : 13 | ['불임_원인_개수', '총시술_ord', 'IVF시술_ord', '클리닉시술_ord', 'DI시술_ord', ...]\n",
      "\t\t('int', ['bool']) : 32 | ['시술 유형', '배란 자극 여부', '배란 유도 유형', '단일 배아 이식 여부', '착상 전 유전 검사 사용 여부', ...]\n",
      "\t7.2s = Fit runtime\n",
      "\t121 features in original data used to generate 121 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 93.50 MB (1.5% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 7.9s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 5337.1s of the 5337.09s of remaining time.\n",
      "\t0.6339\t = Validation score   (roc_auc)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t55.83s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 5280.33s of the 5280.32s of remaining time.\n",
      "\t0.6201\t = Validation score   (roc_auc)\n",
      "\t0.44s\t = Training   runtime\n",
      "\t56.46s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5222.96s of the 5222.95s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.72% memory usage per fold, 50.87%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=12.72%)\n",
      "\t0.7401\t = Validation score   (roc_auc)\n",
      "\t31.5s\t = Training   runtime\n",
      "\t3.59s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 5187.65s of the 5187.64s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.13% memory usage per fold, 44.54%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=11.13%)\n",
      "\t0.7398\t = Validation score   (roc_auc)\n",
      "\t29.67s\t = Training   runtime\n",
      "\t3.53s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 5153.77s of the 5153.77s of remaining time.\n",
      "\t0.7264\t = Validation score   (roc_auc)\n",
      "\t31.05s\t = Training   runtime\n",
      "\t12.64s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 5109.02s of the 5109.01s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 286 due to low memory. Expected memory usage reduced from 15.68% -> 15.0% of available memory...\n",
      "\t0.7275\t = Validation score   (roc_auc)\n",
      "\t28.64s\t = Training   runtime\n",
      "\t17.59s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 5061.53s of the 5061.53s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.45% memory usage per fold, 57.79%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=14.45%)\n",
      "\t0.741\t = Validation score   (roc_auc)\n",
      "\t3848.92s\t = Training   runtime\n",
      "\t0.65s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 1208.82s of the 1208.82s of remaining time.\n",
      "\t0.7297\t = Validation score   (roc_auc)\n",
      "\t23.83s\t = Training   runtime\n",
      "\t18.32s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 1165.37s of the 1165.36s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 273 due to low memory. Expected memory usage reduced from 16.46% -> 15.0% of available memory...\n",
      "\t0.7304\t = Validation score   (roc_auc)\n",
      "\t22.78s\t = Training   runtime\n",
      "\t16.99s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1124.55s of the 1124.54s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 24.59% memory usage per fold, 49.19%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=24.59%)\n",
      "\t0.7373\t = Validation score   (roc_auc)\n",
      "\t872.32s\t = Training   runtime\n",
      "\t6.47s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 247.59s of the 247.59s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.05% memory usage per fold, 40.09%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=20.05%)\n",
      "\t0.7366\t = Validation score   (roc_auc)\n",
      "\t211.84s\t = Training   runtime\n",
      "\t2.29s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 32.15s of the 32.14s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.71% memory usage per fold, 50.83%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=12.71%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "2026-02-11 20:37:25,922\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-02-11 20:37:25,926\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 16.72s of the 16.71s of remaining time.\n",
      "2026-02-11 20:37:25,928\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2026-02-11 20:37:25,929\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.20% memory usage per fold, 60.81%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=15.20%)\n",
      "\t0.7359\t = Validation score   (roc_auc)\n",
      "\t18.77s\t = Training   runtime\n",
      "\t1.19s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 533.71s of the -5.14s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.579, 'LightGBMXT_BAG_L1': 0.158, 'NeuralNetFastAI_BAG_L1': 0.105, 'LightGBM_BAG_L1': 0.053, 'ExtraTreesEntr_BAG_L1': 0.053, 'XGBoost_BAG_L1': 0.053}\n",
      "\t0.7413\t = Validation score   (roc_auc)\n",
      "\t23.43s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5373.75s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../outputs/ag_fold4\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 4 AUC: 0.738970\n",
      "  Best model: WeightedEnsemble_L2 (score=0.741306)\n",
      "\n",
      "--- Fold 5/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 7200 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: ../outputs/ag_fold5/ds_sub_fit/sub_fit_ho.\n",
      "Running the sub-fit in a ray process to avoid memory leakage.\n",
      "Spend 1871 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 5329 seconds.\n",
      "Starting full fit now with num_stack_levels 0.\n",
      "Beginning AutoGluon training ... Time limit = 5329s\n",
      "AutoGluon will save models to \"../outputs/ag_fold5\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.11.14\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 24.6.0: Mon Jul 14 11:30:29 PDT 2025; root:xnu-11417.140.69~1/RELEASE_ARM64_T6000\n",
      "CPU Count:          10\n",
      "Memory Avail:       4.84 GB / 16.00 GB (30.3%)\n",
      "Disk Space Avail:   187.49 GB / 460.43 GB (40.7%)\n",
      "===================================================\n",
      "Train Data Rows:    205081\n",
      "Train Data Columns: 128\n",
      "Label Column:       임신 성공 여부\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5523.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 871.26 MB (15.8% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 15.8% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 35 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 3): ['불임 원인 - 여성 요인', '배아_이식_여부', '배아_해동_실시_여부']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 5): ['불임 원인 - 정자 면역학적 요인', '불임 원인 - 정자 운동성', '신선_배양시간', 'PGD_실시', 'PGS_실시']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 1 | ['신선_배양시간']\n",
      "\t\t('int', [])   : 4 | ['불임 원인 - 정자 면역학적 요인', '불임 원인 - 정자 운동성', 'PGD_실시', 'PGS_실시']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 46 | ['임신 시도 또는 마지막 임신 경과 연수', '단일 배아 이식 여부', '착상 전 유전 검사 사용 여부', '착상 전 유전 진단 사용 여부', '총 생성 배아 수', ...]\n",
      "\t\t('int', [])    : 34 | ['배란 자극 여부', '남성 주 불임 원인', '남성 부 불임 원인', '여성 주 불임 원인', '여성 부 불임 원인', ...]\n",
      "\t\t('object', []) : 40 | ['시술 시기 코드', '시술 당시 나이', '시술 유형', '특정 시술 유형', '배란 유도 유형', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 38 | ['시술 시기 코드', '시술 당시 나이', '특정 시술 유형', '배아 생성 주요 이유', '총 시술 횟수', ...]\n",
      "\t\t('float', [])     : 38 | ['임신 시도 또는 마지막 임신 경과 연수', '착상 전 유전 진단 사용 여부', '총 생성 배아 수', '미세주입된 난자 수', '미세주입에서 생성된 배아 수', ...]\n",
      "\t\t('int', [])       : 13 | ['불임_원인_개수', '총시술_ord', 'IVF시술_ord', '클리닉시술_ord', 'DI시술_ord', ...]\n",
      "\t\t('int', ['bool']) : 31 | ['시술 유형', '배란 자극 여부', '배란 유도 유형', '단일 배아 이식 여부', '착상 전 유전 검사 사용 여부', ...]\n",
      "\t7.2s = Fit runtime\n",
      "\t120 features in original data used to generate 120 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 93.31 MB (1.6% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 7.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 5321.25s of the 5321.22s of remaining time.\n",
      "\t0.6333\t = Validation score   (roc_auc)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t55.0s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 5265.35s of the 5265.33s of remaining time.\n",
      "\t0.6197\t = Validation score   (roc_auc)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t54.12s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5210.47s of the 5210.44s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 13.47% memory usage per fold, 53.88%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=13.47%)\n",
      "\t0.7389\t = Validation score   (roc_auc)\n",
      "\t29.6s\t = Training   runtime\n",
      "\t3.17s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 5176.77s of the 5176.74s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 13.06% memory usage per fold, 52.26%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=13.06%)\n",
      "\t0.7389\t = Validation score   (roc_auc)\n",
      "\t24.54s\t = Training   runtime\n",
      "\t2.05s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 5148.27s of the 5148.25s of remaining time.\n",
      "\t0.726\t = Validation score   (roc_auc)\n",
      "\t28.23s\t = Training   runtime\n",
      "\t17.76s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 5101.22s of the 5101.2s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 282 due to low memory. Expected memory usage reduced from 15.92% -> 15.0% of available memory...\n",
      "\t0.7263\t = Validation score   (roc_auc)\n",
      "\t28.77s\t = Training   runtime\n",
      "\t17.11s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 5054.41s of the 5054.39s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.29% memory usage per fold, 61.15%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=15.29%)\n",
      "\t0.7401\t = Validation score   (roc_auc)\n",
      "\t3701.48s\t = Training   runtime\n",
      "\t0.69s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 1349.21s of the 1349.18s of remaining time.\n",
      "\t0.7287\t = Validation score   (roc_auc)\n",
      "\t15.71s\t = Training   runtime\n",
      "\t10.93s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 1321.63s of the 1321.6s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 260 due to low memory. Expected memory usage reduced from 17.3% -> 15.0% of available memory...\n",
      "\t0.7289\t = Validation score   (roc_auc)\n",
      "\t14.41s\t = Training   runtime\n",
      "\t9.57s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1296.83s of the 1296.8s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 25.25% memory usage per fold, 50.51%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=25.25%)\n",
      "\tWarning: Exception caused NeuralNetFastAI_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=90944, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 404, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 855, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 358, in _fit\n",
      "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
      "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 272, in fit\n",
      "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 207, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "                                        ^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 261, in _do_fit\n",
      "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 209, in _with_events\n",
      "    self(f'after_{event_type}');  final()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 180, in __call__\n",
      "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastcore/foundation.py\", line 163, in map\n",
      "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
      "                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastcore/basics.py\", line 934, in map_ex\n",
      "    return list(res)\n",
      "           ^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastcore/basics.py\", line 919, in __call__\n",
      "    return self.func(*fargs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 184, in _call_one\n",
      "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
      "                                        ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/callback/core.py\", line 64, in __call__\n",
      "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/callback/core.py\", line 62, in __call__\n",
      "    try: res = getcallable(self, event_name)()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
      "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
      "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
      "\tWARNING: NaN loss encountered in epoch 0!\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n",
      "    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 855, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 165, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 286, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 717, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 663, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 605, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 568, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 534, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time = self.ray.get(finished)\n",
      "                                                                                         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/ray/_private/worker.py\", line 2667, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/ray/_private/worker.py\", line 864, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AssertionError): \u001b[36mray::_ray_fit()\u001b[39m (pid=90944, ip=127.0.0.1)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 404, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 855, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/tabular/models/fastainn/tabular_nn_fastai.py\", line 358, in _fit\n",
      "    self.model.fit_one_cycle(epochs, params[\"lr\"], cbs=callbacks)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/callback/schedule.py\", line 121, in fit_one_cycle\n",
      "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 272, in fit\n",
      "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 207, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "                                        ^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 261, in _do_fit\n",
      "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 209, in _with_events\n",
      "    self(f'after_{event_type}');  final()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 180, in __call__\n",
      "    def __call__(self, event_name): L(event_name).map(self._call_one)\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastcore/foundation.py\", line 163, in map\n",
      "    def map(self, f, *args, **kwargs): return self._new(map_ex(self, f, *args, gen=False, **kwargs))\n",
      "                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastcore/basics.py\", line 934, in map_ex\n",
      "    return list(res)\n",
      "           ^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastcore/basics.py\", line 919, in __call__\n",
      "    return self.func(*fargs, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/learner.py\", line 184, in _call_one\n",
      "    for cb in self.cbs.sorted('order'): cb(event_name)\n",
      "                                        ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/callback/core.py\", line 64, in __call__\n",
      "    except Exception as e: raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/fastai/callback/core.py\", line 62, in __call__\n",
      "    try: res = getcallable(self, event_name)()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/fertility_ai/lib/python3.11/site-packages/autogluon/tabular/models/fastainn/callbacks.py\", line 63, in after_epoch\n",
      "    raise AssertionError(f\"WARNING: NaN loss encountered in epoch {self.epoch}!\")\n",
      "AssertionError: Exception occured in `EarlyStoppingCallbackWithTimeLimit` when calling event `after_epoch`:\n",
      "\tWARNING: NaN loss encountered in epoch 0!\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 666.02s of the 665.99s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.52% memory usage per fold, 41.03%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=5, gpus=0, memory=20.52%)\n",
      "2026-02-11 22:29:26,208\tERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t0.7386\t = Validation score   (roc_auc)\n",
      "\t543.05s\t = Training   runtime\n",
      "\t2.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 119.91s of the 119.88s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.01% memory usage per fold, 56.03%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=14.01%)\n",
      "\t0.7362\t = Validation score   (roc_auc)\n",
      "\t93.42s\t = Training   runtime\n",
      "\t6.95s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 22.8s of the 22.77s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.92% memory usage per fold, 67.67%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=2, gpus=0, memory=16.92%)\n",
      "\t0.7377\t = Validation score   (roc_auc)\n",
      "\t21.64s\t = Training   runtime\n",
      "\t2.42s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 532.12s of the -2.11s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.65, 'LightGBM_BAG_L1': 0.15, 'NeuralNetTorch_BAG_L1': 0.1, 'XGBoost_BAG_L1': 0.05, 'LightGBMLarge_BAG_L1': 0.05}\n",
      "\t0.7404\t = Validation score   (roc_auc)\n",
      "\t13.71s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5344.92s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../outputs/ag_fold5\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold 5 AUC: 0.741451\n",
      "  Best model: WeightedEnsemble_L2 (score=0.740358)\n",
      "\n",
      "============================================================\n",
      ">>> AutoGluon OOF AUC = 0.740734\n",
      "============================================================\n",
      "\n",
      "Saved: ../outputs/14_ag_CV0.740734.csv\n",
      "\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 14번: AutoGluon + KNN Imputation + 12번 피처셋\n",
    "# - 12번과 동일한 imputation + 피처 엔지니어링\n",
    "# - AutoGluon이 자체 앙상블 (LGB/CAT/XGB/RF/NN 등)\n",
    "# ============================================================\n",
    "\n",
    "import os, gc, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "DATA_DIR = \"../data\"\n",
    "OUT_DIR  = \"../outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_COL = \"임신 성공 여부\"\n",
    "ID_COL = \"ID\"\n",
    "N_FOLDS = 5\n",
    "SEED = 42\n",
    "\n",
    "# AutoGluon 설정\n",
    "AG_TIME_LIMIT = 7200        # 1시간 (fold당). 늘리면 성능↑\n",
    "AG_PRESETS = \"best_quality\"  # best_quality > high_quality > medium_quality\n",
    "AG_NUM_STACK = 1             # stacking 레벨\n",
    "AG_NUM_BAG = 8               # bagging fold 수\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 1) Imputation (12번과 동일)\n",
    "# ===================================================================\n",
    "def impute_missing(train_df, test_df):\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    di_fill_cols = [\n",
    "        '미세주입된 난자 수', '미세주입에서 생성된 배아 수', '총 생성 배아 수', '이식된 배아 수',\n",
    "        '미세주입 배아 이식 수', '저장된 배아 수', '미세주입 후 저장된 배아 수', '해동된 배아 수',\n",
    "        '해동 난자 수', '수집된 신선 난자 수', '저장된 신선 난자 수', '혼합된 난자 수',\n",
    "        '파트너 정자와 혼합된 난자 수', '기증자 정자와 혼합된 난자 수',\n",
    "        '난자 채취 경과일', '난자 해동 경과일', '배아 이식 경과일', '배아 해동 경과일',\n",
    "        '동결 배아 사용 여부', '신선 배아 사용 여부', '기증 배아 사용 여부',\n",
    "        '단일 배아 이식 여부', '난자 혼합 경과일',\n",
    "        '임신 시도 또는 마지막 임신 경과 연수',\n",
    "    ]\n",
    "    \n",
    "    for df in [train_df, test_df]:\n",
    "        di_mask = df['시술 유형'] == 'DI'\n",
    "        for col in di_fill_cols:\n",
    "            if col in df.columns:\n",
    "                df.loc[di_mask, col] = df.loc[di_mask, col].fillna(0)\n",
    "    \n",
    "    knn_cols = [\n",
    "        '총 생성 배아 수', '미세주입된 난자 수', '미세주입에서 생성된 배아 수',\n",
    "        '이식된 배아 수', '미세주입 배아 이식 수', '저장된 배아 수',\n",
    "        '미세주입 후 저장된 배아 수', '해동된 배아 수', '해동 난자 수',\n",
    "        '수집된 신선 난자 수', '저장된 신선 난자 수', '혼합된 난자 수',\n",
    "        '파트너 정자와 혼합된 난자 수', '기증자 정자와 혼합된 난자 수',\n",
    "        '동결 배아 사용 여부', '신선 배아 사용 여부', '기증 배아 사용 여부',\n",
    "        '단일 배아 이식 여부',\n",
    "        '배아 이식 경과일', '난자 혼합 경과일', '배아 해동 경과일',\n",
    "    ]\n",
    "    knn_cols = [c for c in knn_cols if c in train_df.columns]\n",
    "    has_null = [c for c in knn_cols if train_df[c].isnull().any() or test_df[c].isnull().any()]\n",
    "    \n",
    "    if has_null:\n",
    "        print(f\"  KNN Imputing {len(has_null)} columns...\")\n",
    "        imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "        train_df[has_null] = imputer.fit_transform(train_df[has_null])\n",
    "        test_df[has_null] = imputer.transform(test_df[has_null])\n",
    "        print(\"  Done.\")\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 2) Feature Engineering (12번과 동일)\n",
    "# ===================================================================\n",
    "def preprocess(df):\n",
    "    d = df.copy()\n",
    "\n",
    "    # 시술_대분류\n",
    "    def major_procedure(x):\n",
    "        if pd.isna(x): return \"Unknown\"\n",
    "        if \"IUI\" in x: return \"IUI\"\n",
    "        if \"DI\" in x:  return \"Other\"\n",
    "        if \"ICSI\" in x: return \"ICSI\"\n",
    "        if \"IVF\" in x: return \"IVF\"\n",
    "        return \"Other\"\n",
    "    d[\"시술_대분류\"] = d[\"특정 시술 유형\"].apply(major_procedure)\n",
    "\n",
    "    d[\"BLASTOCYST_포함\"] = d[\"특정 시술 유형\"].astype(str).str.contains(\"BLASTOCYST\", na=False).astype(int)\n",
    "    d[\"AH_포함\"] = d[\"특정 시술 유형\"].astype(str).str.contains(\"AH\", na=False).astype(int)\n",
    "\n",
    "    embryo_stage_cols = [\n",
    "        \"단일 배아 이식 여부\", \"착상 전 유전 진단 사용 여부\", \"배아 생성 주요 이유\",\n",
    "        \"총 생성 배아 수\", \"미세주입된 난자 수\", \"미세주입에서 생성된 배아 수\",\n",
    "        \"이식된 배아 수\", \"미세주입 배아 이식 수\", \"저장된 배아 수\",\n",
    "        \"미세주입 후 저장된 배아 수\", \"해동된 배아 수\", \"해동 난자 수\",\n",
    "        \"수집된 신선 난자 수\", \"저장된 신선 난자 수\", \"혼합된 난자 수\",\n",
    "        \"파트너 정자와 혼합된 난자 수\", \"기증자 정자와 혼합된 난자 수\",\n",
    "        \"동결 배아 사용 여부\", \"신선 배아 사용 여부\", \"기증 배아 사용 여부\", \"대리모 여부\",\n",
    "    ]\n",
    "    d[\"배아_이식_여부\"] = 1 - d[embryo_stage_cols].isna().all(axis=1).astype(int)\n",
    "\n",
    "    def embryo_stage(row):\n",
    "        if row[\"배아_이식_여부\"] == 0: return \"배아단계_미도달\"\n",
    "        elif pd.isna(row[\"총 생성 배아 수\"]) or row[\"총 생성 배아 수\"] == 0: return \"배아생성_실패\"\n",
    "        elif pd.isna(row[\"이식된 배아 수\"]) or row[\"이식된 배아 수\"] == 0: return \"이식_미실시\"\n",
    "        else: return \"이식_완료\"\n",
    "    d[\"배아_진행_단계\"] = d.apply(embryo_stage, axis=1)\n",
    "\n",
    "    def collapse_trials(x):\n",
    "        if x == \"0회\": return \"0회\"\n",
    "        elif x in [\"1회\",\"2회\"]: return \"1–2회\"\n",
    "        else: return \"3회 이상\"\n",
    "    d[\"총시술_bin3\"] = d[\"총 시술 횟수\"].apply(collapse_trials)\n",
    "\n",
    "    def age_group_simple(age):\n",
    "        if age == \"알 수 없음\": return \"Unknown\"\n",
    "        elif age == \"만18-34세\": return \"34세 이하\"\n",
    "        elif age in [\"만35-37세\",\"만38-39세\"]: return \"35-39세\"\n",
    "        else: return \"40세 이상\"\n",
    "    d[\"나이_3구간\"] = d[\"시술 당시 나이\"].apply(age_group_simple)\n",
    "\n",
    "    def embryo_count_bin(count):\n",
    "        if pd.isna(count) or count == 0: return \"0개\"\n",
    "        elif count <= 2: return \"1-2개\"\n",
    "        else: return \"3개 이상\"\n",
    "    d[\"이식배아_구간\"] = d[\"이식된 배아 수\"].apply(embryo_count_bin)\n",
    "\n",
    "    d[\"Day5_이식_여부\"] = (d[\"배아 이식 경과일\"] == 5.0).astype(int)\n",
    "\n",
    "    infertility_cols = [\n",
    "        \"남성 주 불임 원인\", \"남성 부 불임 원인\", \"여성 주 불임 원인\", \"여성 부 불임 원인\",\n",
    "        \"부부 주 불임 원인\", \"부부 부 불임 원인\", \"불명확 불임 원인\",\n",
    "        \"불임 원인 - 난관 질환\", \"불임 원인 - 남성 요인\", \"불임 원인 - 배란 장애\",\n",
    "        \"불임 원인 - 여성 요인\", \"불임 원인 - 자궁경부 문제\", \"불임 원인 - 자궁내막증\",\n",
    "        \"불임 원인 - 정자 농도\", \"불임 원인 - 정자 면역학적 요인\", \"불임 원인 - 정자 운동성\",\n",
    "        \"불임 원인 - 정자 형태\"\n",
    "    ]\n",
    "    d[\"불임_원인_개수\"] = d[infertility_cols].sum(axis=1)\n",
    "\n",
    "    d[\"배아_해동_실시_여부\"] = d[\"배아 해동 경과일\"].notna().astype(int)\n",
    "\n",
    "    # 비율\n",
    "    d[\"배아_이식_비율\"] = d[\"이식된 배아 수\"] / (d[\"총 생성 배아 수\"] + 1)\n",
    "    d[\"배아_저장_비율\"] = d[\"저장된 배아 수\"] / (d[\"총 생성 배아 수\"] + 1)\n",
    "    d[\"배아_생성_효율\"] = d[\"총 생성 배아 수\"] / (d[\"수집된 신선 난자 수\"] + 1)\n",
    "    d[\"미세주입_생성_효율\"] = d[\"미세주입에서 생성된 배아 수\"] / (d[\"미세주입된 난자 수\"] + 1)\n",
    "    d[\"난자_활용률\"] = d[\"혼합된 난자 수\"] / (d[\"수집된 신선 난자 수\"] + 1)\n",
    "\n",
    "    # 교호작용\n",
    "    d[\"나이×Day5\"] = d[\"시술 당시 나이\"].astype(str) + \"_\" + d[\"Day5_이식_여부\"].astype(str)\n",
    "    d[\"시술횟수×나이\"] = d[\"총시술_bin3\"].astype(str) + \"_\" + d[\"나이_3구간\"].astype(str)\n",
    "    d[\"나이×배아진행\"] = d[\"시술 당시 나이\"].astype(str) + \"_\" + d[\"배아_진행_단계\"]\n",
    "    d[\"시기코드×나이\"] = d[\"시술 시기 코드\"] + \"_\" + d[\"나이_3구간\"]\n",
    "    d[\"나이×단일이식\"] = d[\"시술 당시 나이\"].astype(str) + \"_\" + d[\"단일 배아 이식 여부\"].fillna(-1).astype(int).astype(str)\n",
    "\n",
    "    # 배양기간\n",
    "    d[\"배양기간\"] = d[\"배아 이식 경과일\"] - d[\"난자 혼합 경과일\"]\n",
    "\n",
    "    # ordinal\n",
    "    ord_map = {\"0회\":0, \"1회\":1, \"2회\":2, \"3회\":3, \"4회\":4, \"5회\":5, \"6회 이상\":6}\n",
    "    for col_name, col_src in [(\"총시술_ord\",\"총 시술 횟수\"), (\"IVF시술_ord\",\"IVF 시술 횟수\"),\n",
    "                               (\"클리닉시술_ord\",\"클리닉 내 총 시술 횟수\"), (\"DI시술_ord\",\"DI 시술 횟수\"),\n",
    "                               (\"총임신_ord\",\"총 임신 횟수\"), (\"IVF임신_ord\",\"IVF 임신 횟수\"),\n",
    "                               (\"총출산_ord\",\"총 출산 횟수\"), (\"IVF출산_ord\",\"IVF 출산 횟수\"),\n",
    "                               (\"DI임신_ord\",\"DI 임신 횟수\"), (\"DI출산_ord\",\"DI 출산 횟수\")]:\n",
    "        d[col_name] = d[col_src].map(ord_map)\n",
    "\n",
    "    # 난자 나이\n",
    "    def get_egg_age(row):\n",
    "        if row[\"난자 출처\"] == \"본인 제공\":\n",
    "            return row[\"시술 당시 나이\"]\n",
    "        elif row[\"난자 출처\"] == \"기증 제공\":\n",
    "            donor = row[\"난자 기증자 나이\"]\n",
    "            return donor if donor != \"알 수 없음\" else \"만18-34세\"\n",
    "        return row[\"시술 당시 나이\"]\n",
    "    d[\"난자_나이\"] = d.apply(get_egg_age, axis=1)\n",
    "\n",
    "    # 배아 출처\n",
    "    def get_embryo_source(row):\n",
    "        fr = 0 if pd.isna(row.get(\"동결 배아 사용 여부\")) else int(row[\"동결 배아 사용 여부\"])\n",
    "        fs = 0 if pd.isna(row.get(\"신선 배아 사용 여부\")) else int(row[\"신선 배아 사용 여부\"])\n",
    "        if fr and fs: return \"both\"\n",
    "        if fr: return \"frozen\"\n",
    "        if fs: return \"fresh\"\n",
    "        return \"none\"\n",
    "    d[\"배아_출처\"] = d.apply(get_embryo_source, axis=1)\n",
    "\n",
    "    d[\"시기×배아출처\"] = d[\"시술 시기 코드\"] + \"_\" + d[\"배아_출처\"]\n",
    "\n",
    "    # 배아 생성 이유\n",
    "    def embryo_reason_simple(x):\n",
    "        if pd.isna(x): return \"미도달\"\n",
    "        if \"현재 시술용\" in x: return \"시술용\"\n",
    "        return \"저장기증\"\n",
    "    d[\"배아생성이유\"] = d[\"배아 생성 주요 이유\"].apply(embryo_reason_simple)\n",
    "\n",
    "    # 배란\n",
    "    d[\"배란 유도 유형\"] = d[\"배란 유도 유형\"].replace({\n",
    "        '생식선 자극 호르몬': '기록되지 않은 시행', '세트로타이드 (억제제)': '기록되지 않은 시행'\n",
    "    })\n",
    "    d[\"배란_자극_유도\"] = d[\"배란 자극 여부\"].astype(str) + \"_\" + d[\"배란 유도 유형\"].astype(str)\n",
    "\n",
    "    d[\"나이_알수없음\"] = (d[\"시술 당시 나이\"] == \"알 수 없음\").astype(int)\n",
    "\n",
    "    # 불임 남/여 분리\n",
    "    male_inf = [\"불임 원인 - 남성 요인\", \"불임 원인 - 정자 농도\",\n",
    "                \"불임 원인 - 정자 면역학적 요인\", \"불임 원인 - 정자 운동성\", \"불임 원인 - 정자 형태\"]\n",
    "    female_inf = [\"불임 원인 - 난관 질환\", \"불임 원인 - 배란 장애\",\n",
    "                  \"불임 원인 - 자궁경부 문제\", \"불임 원인 - 자궁내막증\"]\n",
    "    d[\"남성_불임_수\"] = d[male_inf].sum(axis=1)\n",
    "    d[\"여성_불임_수\"] = d[female_inf].sum(axis=1)\n",
    "    d[\"여성_불임_비율\"] = np.where(\n",
    "        (d[\"남성_불임_수\"] + d[\"여성_불임_수\"]) == 0, -1,\n",
    "        d[\"여성_불임_수\"] / (d[\"남성_불임_수\"] + d[\"여성_불임_수\"])\n",
    "    )\n",
    "\n",
    "    # 시술 이력 비율\n",
    "    d[\"IVF_임신률\"] = np.where(d[\"IVF시술_ord\"] == 0, -1, d[\"IVF임신_ord\"] / d[\"IVF시술_ord\"])\n",
    "    d[\"IVF_출산률\"] = np.where(d[\"IVF임신_ord\"] == 0, -1, d[\"IVF출산_ord\"] / d[\"IVF임신_ord\"])\n",
    "    d[\"총_출산률\"] = np.where(d[\"총임신_ord\"] == 0, -1, d[\"총출산_ord\"] / d[\"총임신_ord\"])\n",
    "    d[\"클리닉_비율\"] = np.where(d[\"총시술_ord\"] == 0, -1, d[\"클리닉시술_ord\"] / d[\"총시술_ord\"])\n",
    "\n",
    "    # 추가 비율\n",
    "    d[\"미세주입_이식_비율\"] = np.where(d[\"이식된 배아 수\"] == 0, -1, d[\"미세주입 배아 이식 수\"] / d[\"이식된 배아 수\"])\n",
    "    d[\"혼합_생성률\"] = np.where((d[\"혼합된 난자 수\"] + d[\"해동 난자 수\"]) == 0, -1,\n",
    "                              d[\"총 생성 배아 수\"] / (d[\"혼합된 난자 수\"] + d[\"해동 난자 수\"]))\n",
    "    d[\"미세주입_배아_생성률\"] = np.where(d[\"미세주입된 난자 수\"] == 0, -1,\n",
    "                                     d[\"미세주입에서 생성된 배아 수\"] / d[\"미세주입된 난자 수\"])\n",
    "    d[\"총_사용_배아\"] = d[\"해동된 배아 수\"] + d[\"총 생성 배아 수\"]\n",
    "    d[\"IVF_정자_미혼합\"] = ((d[\"파트너 정자와 혼합된 난자 수\"] == 0) &\n",
    "                          (d[\"기증자 정자와 혼합된 난자 수\"] == 0) &\n",
    "                          (d[\"시술 유형\"] == \"IVF\")).astype(int)\n",
    "\n",
    "    # 시기별 세분화\n",
    "    d[\"시기×단일이식\"] = d[\"시술 시기 코드\"] + \"_\" + d[\"단일 배아 이식 여부\"].fillna(0).astype(int).astype(str)\n",
    "    d[\"시기×배란자극\"] = d[\"시술 시기 코드\"] + \"_\" + d[\"배란 자극 여부\"].astype(str)\n",
    "    d[\"시기×유전진단\"] = d[\"시술 시기 코드\"] + \"_\" + d[\"착상 전 유전 진단 사용 여부\"].fillna(0).astype(int).astype(str)\n",
    "\n",
    "    # 배양 시간\n",
    "    d[\"신선_배양시간\"] = (d[\"배아 이식 경과일\"] - d[\"난자 혼합 경과일\"]).fillna(0)\n",
    "    d[\"동결_배양시간\"] = (d[\"배아 이식 경과일\"] - d[\"배아 해동 경과일\"]).fillna(0)\n",
    "    d[\"이상적_배양\"] = d[\"배아 이식 경과일\"].isin([3.0, 5.0]).astype(int)\n",
    "\n",
    "    # 시기별 신선난자/해동배아 구간\n",
    "    def fresh_egg_tier(row):\n",
    "        if pd.isna(row[\"신선 배아 사용 여부\"]) or row[\"신선 배아 사용 여부\"] == 0:\n",
    "            return \"not_fresh\"\n",
    "        eggs = row[\"수집된 신선 난자 수\"] if not pd.isna(row[\"수집된 신선 난자 수\"]) else 0\n",
    "        code = row[\"시술 시기 코드\"]\n",
    "        if eggs > 10: return f\"{code}_high\"\n",
    "        elif eggs > 0: return f\"{code}_low\"\n",
    "        return f\"{code}_zero\"\n",
    "    d[\"시기별_신선난자_구간\"] = d.apply(fresh_egg_tier, axis=1)\n",
    "\n",
    "    def frozen_thaw_tier(row):\n",
    "        if pd.isna(row[\"동결 배아 사용 여부\"]) or row[\"동결 배아 사용 여부\"] == 0:\n",
    "            return \"not_frozen\"\n",
    "        thawed = row[\"해동된 배아 수\"] if not pd.isna(row[\"해동된 배아 수\"]) else 0\n",
    "        code = row[\"시술 시기 코드\"]\n",
    "        if thawed > 3: return f\"{code}_many\"\n",
    "        elif thawed > 0: return f\"{code}_few\"\n",
    "        return f\"{code}_zero\"\n",
    "    d[\"시기별_해동배아_구간\"] = d.apply(frozen_thaw_tier, axis=1)\n",
    "\n",
    "    # PGD/PGS\n",
    "    d[\"유전검사_합\"] = d[\"착상 전 유전 진단 사용 여부\"].fillna(0) + d[\"착상 전 유전 검사 사용 여부\"].fillna(0)\n",
    "    d[\"PGD_실시\"] = d[\"PGD 시술 여부\"].notna().astype(int)\n",
    "    d[\"PGS_실시\"] = d[\"PGS 시술 여부\"].notna().astype(int)\n",
    "    d[\"대리모 여부\"] = d[\"대리모 여부\"].fillna(-1)\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 3) Main: OOF AutoGluon\n",
    "# ===================================================================\n",
    "def main():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"14번: AutoGluon + KNN Imputation + 12번 피처셋\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Load\n",
    "    train_raw = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "    test_raw  = pd.read_csv(os.path.join(DATA_DIR, \"test.csv\"))\n",
    "    sub       = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\n",
    "\n",
    "    # Step 1: Imputation\n",
    "    print(\"\\n[Step 1] Imputation...\")\n",
    "    train_raw, test_raw = impute_missing(train_raw, test_raw)\n",
    "\n",
    "    # Step 2: Feature Engineering\n",
    "    print(\"\\n[Step 2] Feature Engineering...\")\n",
    "    train_df = preprocess(train_raw)\n",
    "    test_df  = preprocess(test_raw)\n",
    "\n",
    "    # Drop\n",
    "    drop_cols = [ID_COL]\n",
    "    train_df = train_df.drop(columns=[c for c in drop_cols if c in train_df.columns])\n",
    "    test_df  = test_df.drop(columns=[c for c in drop_cols if c in test_df.columns])\n",
    "\n",
    "    feature_cols = [c for c in train_df.columns if c != TARGET_COL]\n",
    "    print(f\"Features: {len(feature_cols)}\")\n",
    "\n",
    "    # Step 3: OOF AutoGluon\n",
    "    print(f\"\\n[Step 3] AutoGluon OOF ({N_FOLDS}-fold)\")\n",
    "    print(f\"  presets={AG_PRESETS}, time_limit={AG_TIME_LIMIT}s/fold\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    y = train_df[TARGET_COL].values\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    oof_pred = np.zeros(len(train_df))\n",
    "    test_pred = np.zeros(len(test_df))\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(train_df, y), 1):\n",
    "        print(f\"\\n--- Fold {fold}/{N_FOLDS} ---\")\n",
    "\n",
    "        tr_data = train_df.iloc[tr_idx].reset_index(drop=True)\n",
    "        va_data = train_df.iloc[va_idx].reset_index(drop=True)\n",
    "\n",
    "        # AutoGluon 학습\n",
    "        ag_path = os.path.join(OUT_DIR, f\"ag_fold{fold}\")\n",
    "\n",
    "        predictor = TabularPredictor(\n",
    "            label=TARGET_COL,\n",
    "            eval_metric='roc_auc',\n",
    "            path=ag_path,\n",
    "        )\n",
    "\n",
    "        predictor.fit(\n",
    "            train_data=tr_data,\n",
    "            time_limit=AG_TIME_LIMIT,\n",
    "            presets=AG_PRESETS,\n",
    "            num_stack_levels=AG_NUM_STACK,\n",
    "            num_bag_folds=AG_NUM_BAG,\n",
    "            verbosity=2,\n",
    "        )\n",
    "\n",
    "        # OOF prediction\n",
    "        va_pred = predictor.predict_proba(va_data)[1].values\n",
    "        oof_pred[va_idx] = va_pred\n",
    "\n",
    "        fold_auc = roc_auc_score(y[va_idx], va_pred)\n",
    "        print(f\"  Fold {fold} AUC: {fold_auc:.6f}\")\n",
    "\n",
    "        # Test prediction\n",
    "        te_pred = predictor.predict_proba(test_df)[1].values\n",
    "        test_pred += te_pred / N_FOLDS\n",
    "\n",
    "        # Leaderboard\n",
    "        lb = predictor.leaderboard(va_data, silent=True)\n",
    "        print(f\"  Best model: {lb.iloc[0]['model']} (score={lb.iloc[0]['score_val']:.6f})\")\n",
    "\n",
    "        # 메모리 정리\n",
    "        del predictor\n",
    "        gc.collect()\n",
    "\n",
    "    # 결과\n",
    "    oof_auc = roc_auc_score(y, oof_pred)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\">>> AutoGluon OOF AUC = {oof_auc:.6f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Save\n",
    "    sub2 = sub.copy()\n",
    "    sub2[\"probability\"] = test_pred\n",
    "    sub_path = os.path.join(OUT_DIR, f\"14_ag_CV{oof_auc:.6f}.csv\")\n",
    "    sub2.to_csv(sub_path, index=False)\n",
    "    print(f\"\\nSaved: {sub_path}\")\n",
    "\n",
    "    np.save(os.path.join(OUT_DIR, \"14_oof_ag.npy\"), oof_pred)\n",
    "    np.save(os.path.join(OUT_DIR, \"14_test_ag.npy\"), test_pred)\n",
    "\n",
    "    with open(os.path.join(OUT_DIR, \"14_summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"14번 AutoGluon 파이프라인\\n{'='*60}\\n\")\n",
    "        f.write(f\"OOF AUC: {oof_auc:.6f}\\n\")\n",
    "        f.write(f\"N_FOLDS: {N_FOLDS}\\n\")\n",
    "        f.write(f\"AG_TIME_LIMIT: {AG_TIME_LIMIT}s\\n\")\n",
    "        f.write(f\"AG_PRESETS: {AG_PRESETS}\\n\")\n",
    "        f.write(f\"Features: {len(feature_cols)}\\n\")\n",
    "\n",
    "    print(\"\\nDONE!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fertility_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
