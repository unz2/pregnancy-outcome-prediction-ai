{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bcd3579-c806-410f-bddc-973238dcb4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Preprocessing Data...\n",
      "Starting Cross Validation...\n",
      "=== Fold 1 / 5 ===\n",
      "LGBM AUC: 0.73121\n",
      "XGB AUC: 0.73636\n",
      "NN AUC: 0.62289\n",
      "=== Fold 2 / 5 ===\n",
      "LGBM AUC: 0.73633\n",
      "XGB AUC: 0.74132\n",
      "NN AUC: 0.62738\n",
      "=== Fold 3 / 5 ===\n",
      "LGBM AUC: 0.73229\n",
      "XGB AUC: 0.73840\n",
      "NN AUC: 0.63131\n",
      "=== Fold 4 / 5 ===\n",
      "LGBM AUC: 0.73200\n",
      "XGB AUC: 0.73740\n",
      "NN AUC: 0.62247\n",
      "=== Fold 5 / 5 ===\n",
      "LGBM AUC: 0.73397\n",
      "XGB AUC: 0.73927\n",
      "NN AUC: 0.62033\n",
      "\n",
      "Final OOF AUC Scores -> LGBM: 0.73315, XGB: 0.73852, NN: 0.53154\n",
      "Weights -> LGBM: 0.37, XGB: 0.37, NN: 0.27\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 경고 무시\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 랜덤 시드 고정\n",
    "def seed_everything(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "# MPS 장치 설정 (Apple Silicon 가속)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. 데이터 로드\n",
    "# ==========================================\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# ==========================================\n",
    "# 2. 사용자 정의 전처리 함수\n",
    "# ==========================================\n",
    "def preprocess_data(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. 횟수 관련 컬럼 매핑\n",
    "    count_cols = [\n",
    "        '총 시술 횟수', '클리닉 내 총 시술 횟수', \n",
    "        '총 임신 횟수', '총 출산 횟수',\n",
    "        'IVF 시술 횟수', 'DI 시술 횟수',\n",
    "        'IVF 임신 횟수', 'DI 임신 횟수',\n",
    "        'IVF 출산 횟수', 'DI 출산 횟수'\n",
    "    ]\n",
    "    \n",
    "    def map_count_str(x):\n",
    "        if pd.isna(x): return np.nan\n",
    "        x = str(x)\n",
    "        if '6회 이상' in x: return 6\n",
    "        try:\n",
    "            return int(x.replace('회', ''))\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    for col in count_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(map_count_str)\n",
    "        \n",
    "    # 2. 나이 범주형 -> 수치형 매핑\n",
    "    age_map = {\n",
    "        '만18-34세': 2, '만35-37세': 3, '만38-39세': 4, \n",
    "        '만40-42세': 5, '만43-44세': 6, '만45-50세': 7, \n",
    "        '알 수 없음': 1\n",
    "    }\n",
    "    if '시술 당시 나이' in df.columns:\n",
    "        df['나이_코드'] = df['시술 당시 나이'].map(age_map)\n",
    "    \n",
    "    # 3. 시술 유형 그룹화\n",
    "    def clean_type(x):\n",
    "        x = str(x).upper()\n",
    "        if 'ICSI' in x: return 'ICSI'\n",
    "        if 'IVF' in x: return 'IVF'\n",
    "        if 'IUI' in x: return 'IUI'\n",
    "        return 'Other'\n",
    "    \n",
    "    if '특정 시술 유형' in df.columns:\n",
    "        df['시술_유형_그룹'] = df['특정 시술 유형'].apply(clean_type)\n",
    "\n",
    "    # 4. 배아 생성 주요 이유 그룹화\n",
    "    def binning_reason(reason):\n",
    "        if reason == '현재 시술용':\n",
    "            return 'Direct_Treatment'\n",
    "        elif '현재 시술용' in reason:\n",
    "            return 'Mixed_Purpose'\n",
    "        elif any(keyword in reason for keyword in ['기증용', '저장용']):\n",
    "            return 'Storage_Donation'\n",
    "        else:\n",
    "            return 'Others'\n",
    "    \n",
    "    if '배아 생성 주요 이유' in df.columns:\n",
    "        df['배아 생성 주요 이유'] = df['배아 생성 주요 이유'].fillna('Unknown')\n",
    "        df['배아 생성 주요 이유'] = df['배아 생성 주요 이유'].apply(binning_reason)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ==========================================\n",
    "# 3. 파생변수 생성\n",
    "# ==========================================\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 컬럼 존재 여부 확인 후 계산\n",
    "    def get_col(col_name):\n",
    "        return df[col_name] if col_name in df.columns else pd.Series(0, index=df.index)\n",
    "\n",
    "    # --- A. 상호작용 지표 ---\n",
    "    if '나이_코드' in df.columns and '이식된 배아 수' in df.columns:\n",
    "        df['나이x배아'] = df['나이_코드'] * df['이식된 배아 수'].fillna(0)\n",
    "    \n",
    "    # --- B. 불임 원인 복합 지표 ---\n",
    "    infertility_cols = [col for col in df.columns if '불임 원인' in col]\n",
    "    if infertility_cols:\n",
    "        df['총_불임_원인_수'] = df[infertility_cols].sum(axis=1)\n",
    "    \n",
    "    # --- C. 난자/배아 효율성 지표 ---\n",
    "    oocyte_cols = ['수집된 신선 난자 수', '혼합된 난자 수', '기증자 정자와 혼합된 난자 수', '해동 난자 수']\n",
    "    valid_oocyte_cols = [c for c in oocyte_cols if c in df.columns]\n",
    "    \n",
    "    df['총_난자_수'] = df[valid_oocyte_cols].fillna(0).sum(axis=1)\n",
    "    \n",
    "    df['배아_생성_효율'] = get_col('총 생성 배아 수') / (df['총_난자_수'] + 1e-6)\n",
    "    df['이식_효율'] = get_col('이식된 배아 수') / (get_col('총 생성 배아 수') + 1e-6)\n",
    "    df['저장_비율'] = get_col('저장된 배아 수') / (get_col('총 생성 배아 수') + 1e-6)\n",
    "    df['미세주입_성공률'] = get_col('미세주입에서 생성된 배아 수') / (get_col('미세주입된 난자 수') + 1e-6)\n",
    "\n",
    "    # --- D. 과거 성공 이력 ---\n",
    "    df['과거_임신_성공률'] = get_col('총 임신 횟수') / (get_col('총 시술 횟수') + 1e-6)\n",
    "    df['과거_출산_성공률'] = get_col('총 출산 횟수') / (get_col('총 시술 횟수') + 1e-6)\n",
    "    \n",
    "    # --- E. 로그 변환 ---\n",
    "    skewed_cols = ['총 생성 배아 수', '총_난자_수', '총 시술 횟수']\n",
    "    for col in skewed_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = np.log1p(df[col].fillna(0))\n",
    "            \n",
    "    # --- F. 불필요 컬럼 제거 ---\n",
    "    drop_cols = ['ID', '시술 시기 코드', '특정 시술 유형', '시술 유형']\n",
    "    df = df.drop([c for c in drop_cols if c in df.columns], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 전처리 적용\n",
    "print(\"Preprocessing Data...\")\n",
    "train_prep = create_features(preprocess_data(train))\n",
    "test_prep = create_features(preprocess_data(test))\n",
    "\n",
    "# Target 설정\n",
    "target_col = '임신 성공 여부' \n",
    "if target_col not in train_prep.columns:\n",
    "    target_col = train_prep.columns[-1]\n",
    "\n",
    "X = train_prep.drop(columns=[target_col])\n",
    "y = train_prep[target_col]\n",
    "X_test = test_prep.copy()\n",
    "\n",
    "# 컬럼 순서 및 일치 확인\n",
    "X_test = X_test[X.columns]\n",
    "\n",
    "# ==========================================\n",
    "# 4. 데이터 누수 방지 파이프라인 구축\n",
    "# ==========================================\n",
    "\n",
    "# 수치형/범주형 컬럼 분류\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# 전처리기 정의 (Train 통계만 활용)\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # 결측치는 Train 중앙값으로\n",
    "    ('scaler', StandardScaler())                    # 스케일링은 Train Mean/Std로\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Test에 없는 범주는 무시\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# ==========================================\n",
    "# 5. 모델 정의 (Neural Network & Boosters)\n",
    "# ==========================================\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.output = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "def train_nn_model(X_train, y_train, X_val, y_val, input_dim):\n",
    "    model = SimpleNN(input_dim).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "    \n",
    "    # 데이터셋 생성 시 .values를 명시하여 numpy array로 변환 후 텐서화\n",
    "    train_tensor = TensorDataset(torch.FloatTensor(X_train).to(device), \n",
    "                                 torch.FloatTensor(y_train).unsqueeze(1).to(device))\n",
    "    \n",
    "    val_tensor = TensorDataset(torch.FloatTensor(X_val).to(device), \n",
    "                               torch.FloatTensor(y_val.values).unsqueeze(1).to(device)) \n",
    "    \n",
    "    train_loader = DataLoader(train_tensor, batch_size=1024, shuffle=True)\n",
    "    \n",
    "    best_auc = 0\n",
    "    best_model_state = None\n",
    "    early_stopping = 10\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_inputs = torch.FloatTensor(X_val).to(device)\n",
    "            val_preds = model(val_inputs).cpu().numpy().flatten()\n",
    "            val_auc = roc_auc_score(y_val, val_preds)\n",
    "        \n",
    "        scheduler.step(val_auc)\n",
    "        \n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            best_model_state = model.state_dict()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= early_stopping:\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "# ==========================================\n",
    "# 6. 학습 및 앙상블 (Stratified K-Fold)\n",
    "# ==========================================\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# 결과 저장\n",
    "oof_preds_lgbm = np.zeros(X.shape[0])\n",
    "oof_preds_xgb = np.zeros(X.shape[0])\n",
    "oof_preds_nn = np.zeros(X.shape[0])\n",
    "\n",
    "test_preds_lgbm = np.zeros(X_test.shape[0])\n",
    "test_preds_xgb = np.zeros(X_test.shape[0])\n",
    "test_preds_nn = np.zeros(X_test.shape[0])\n",
    "\n",
    "# 불균형 비율 계산\n",
    "scale_pos_weight = 190123 / 66228\n",
    "\n",
    "print(\"Starting Cross Validation...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"=== Fold {fold+1} / {n_splits} ===\")\n",
    "    \n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # 1. 데이터 누수 방지: Fit on Train -> Transform Val & Test\n",
    "    X_train_trans = preprocessor.fit_transform(X_train_fold)\n",
    "    X_val_trans = preprocessor.transform(X_val_fold)\n",
    "    X_test_trans = preprocessor.transform(X_test)\n",
    "    \n",
    "    # --- LGBM ---\n",
    "    lgbm = LGBMClassifier(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=31,\n",
    "        max_depth=-1,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        verbosity=-1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    lgbm.fit(\n",
    "        X_train_trans, y_train,\n",
    "        eval_set=[(X_val_trans, y_val)],\n",
    "        eval_metric='auc'\n",
    "    )\n",
    "    oof_preds_lgbm[val_idx] = lgbm.predict_proba(X_val_trans)[:, 1]\n",
    "    test_preds_lgbm += lgbm.predict_proba(X_test_trans)[:, 1] / n_splits\n",
    "    print(f\"LGBM AUC: {roc_auc_score(y_val, oof_preds_lgbm[val_idx]):.5f}\")\n",
    "\n",
    "    # --- XGB ---\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=2000,\n",
    "        learning_rate=0.03,\n",
    "        max_depth=6,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        tree_method='hist',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='auc',\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "    xgb.fit(\n",
    "        X_train_trans, y_train,\n",
    "        eval_set=[(X_val_trans, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    oof_preds_xgb[val_idx] = xgb.predict_proba(X_val_trans)[:, 1]\n",
    "    test_preds_xgb += xgb.predict_proba(X_test_trans)[:, 1] / n_splits\n",
    "    print(f\"XGB AUC: {roc_auc_score(y_val, oof_preds_xgb[val_idx]):.5f}\")\n",
    "\n",
    "    # --- NN (PyTorch MPS) ---\n",
    "    input_dim = X_train_trans.shape[1]\n",
    "    nn_model = train_nn_model(X_train_trans, y_train.values, X_val_trans, y_val, input_dim)\n",
    "    \n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_tensor = torch.FloatTensor(X_val_trans).to(device)\n",
    "        test_tensor = torch.FloatTensor(X_test_trans).to(device)\n",
    "        \n",
    "        oof_preds_nn[val_idx] = nn_model(val_tensor).cpu().numpy().flatten()\n",
    "        test_preds_nn += nn_model(test_tensor).cpu().numpy().flatten() / n_splits\n",
    "    print(f\"NN AUC: {roc_auc_score(y_val, oof_preds_nn[val_idx]):.5f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 7. 앙상블 및 제출 파일 생성\n",
    "# ==========================================\n",
    "\n",
    "# 모델별 전체 OOF Score 확인\n",
    "auc_lgbm = roc_auc_score(y, oof_preds_lgbm)\n",
    "auc_xgb = roc_auc_score(y, oof_preds_xgb)\n",
    "auc_nn = roc_auc_score(y, oof_preds_nn)\n",
    "\n",
    "print(f\"\\nFinal OOF AUC Scores -> LGBM: {auc_lgbm:.5f}, XGB: {auc_xgb:.5f}, NN: {auc_nn:.5f}\")\n",
    "\n",
    "# 가중 평균 앙상블 (성능 기반 가중치 부여)\n",
    "# NN이 정규화된 데이터에서 좋은 성능을 보일 경우 가중치를 높임\n",
    "total_auc = auc_lgbm + auc_xgb + auc_nn\n",
    "w_lgbm = auc_lgbm / total_auc\n",
    "w_xgb = auc_xgb / total_auc\n",
    "w_nn = auc_nn / total_auc\n",
    "\n",
    "print(f\"Weights -> LGBM: {w_lgbm:.2f}, XGB: {w_xgb:.2f}, NN: {w_nn:.2f}\")\n",
    "\n",
    "final_preds = (test_preds_lgbm * w_lgbm) + (test_preds_xgb * w_xgb) + (test_preds_nn * w_nn)\n",
    "\n",
    "# 제출 파일 저장\n",
    "submission['probability'] = final_preds\n",
    "submission.to_csv('submission_ensemble.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission_ensemble.csv' created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
